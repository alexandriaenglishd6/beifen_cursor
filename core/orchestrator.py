# -*- coding: utf-8 -*-
"""
core.orchestrator â€” ä¸»æµç¨‹ç¼–æ’ï¼ˆå¢é‡/æ‰¹å¤„ç†/è®°å½•/å›å†™ last_seen + Webhookï¼‰
"""
from __future__ import annotations
import os, json, time, threading, hashlib, hmac, logging
from pathlib import Path
from typing import List, Dict, Any, Callable

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# ========== Webhook é€šçŸ¥è¾…åŠ©å‡½æ•° ==========
def _send_webhook_with_retry(url: str, payload: dict, timeout: int = 5, max_retry: int = 3, secret: str = ""):
    """
    å‘é€ webhook è¯·æ±‚ï¼ˆå¸¦é‡è¯• + HMACç­¾åï¼‰
    
    é‡è¯•ç­–ç•¥ï¼š0.5s â†’ 1.5s â†’ 3.5sï¼ˆæŒ‡æ•°é€€é¿ + jitterï¼‰
    """
    try:
        import requests
    except:
        return
    
    import random
    body = json.dumps(payload, ensure_ascii=False).encode("utf-8")
    headers = {"Content-Type": "application/json"}
    
    # HMAC ç­¾åï¼ˆå¯é€‰ï¼‰
    if secret:
        sig = hmac.new(secret.encode(), body, hashlib.sha256).hexdigest()
        headers["X-GPTTool-Signature"] = f"sha256={sig}"
    
    for attempt in range(max_retry):
        try:
            resp = requests.post(url, data=body, headers=headers, timeout=timeout)
            if 200 <= resp.status_code < 300:
                return  # æˆåŠŸ
            if attempt < max_retry - 1:
                backoff = 0.5 * (3 ** attempt) * (1.0 + random.uniform(-0.15, 0.15))
                time.sleep(backoff)
        except Exception:
            if attempt < max_retry - 1:
                backoff = 0.5 * (3 ** attempt) * (1.0 + random.uniform(-0.15, 0.15))
                time.sleep(backoff)
from .detection import detect_links, extract_all_video_urls_from_channel_or_playlist
from .download import download_subtitles
from .net import build_proxy_pool, RateLimiter, CircuitBreaker
from .utils import normalize_url, channel_index_load, channel_index_save, history_append, _ts_utc, ensure_channel_videos_url

# Step 3 Day1: å†å²è®°å½•å’ŒæŠ¥å‘Šç”Ÿæˆ
# Day2: å¢å¼ºé”™è¯¯åˆ†ç±»
# Day3: æ–°å¢ read_history ç”¨äºé‡è¯•åŠŸèƒ½
try:
    from history_schema import write_history, classify_status, classify_status_v2, HistoryRow, read_history
    from report_gen import generate_report
    _HISTORY_AVAILABLE = True
except ImportError:
    _HISTORY_AVAILABLE = False

# ---------- Dry é¢„è§ˆæŠ¥å‘Šç”Ÿæˆ ----------
def _generate_dry_preview(run_dir: str, results: list[dict]) -> str:
    """
    ç”Ÿæˆ Dry Run é¢„è§ˆæŠ¥å‘Šï¼ˆplan.mdï¼‰
    
    è¿”å›ï¼šplan.md è·¯å¾„
    """
    total = len(results)
    has_subs = sum(1 for r in results if r.get("status") == "has_subs")
    no_subs = sum(1 for r in results if r.get("status") == "no_subs")
    errors = sum(1 for r in results if str(r.get("status", "")).startswith("error"))
    
    # è¯­è¨€ç»Ÿè®¡
    lang_counts = {}
    for r in results:
        for lang in r.get("all_langs", []):
            lang_counts[lang] = lang_counts.get(lang, 0) + 1
    
    md = f"""# Dry Run Preview Report

**è¿è¡Œç›®å½•**ï¼š{run_dir}  
**ç”Ÿæˆæ—¶é—´**ï¼š{_ts_utc()}  
**æ¨¡å¼**ï¼šé¢„è§ˆï¼ˆæœªå®é™…ä¸‹è½½/AIå¤„ç†ï¼‰

## ğŸ“Š æ£€æµ‹ç»“æœç»Ÿè®¡

- **æ€»è§†é¢‘æ•°**ï¼š{total}
- **æœ‰å­—å¹•**ï¼š{has_subs} ({has_subs/total*100:.1f}%)
- **æ— å­—å¹•**ï¼š{no_subs} ({no_subs/total*100:.1f}%)
- **é”™è¯¯**ï¼š{errors} ({errors/total*100:.1f}%)

## ğŸŒ è¯­è¨€åˆ†å¸ƒ

"""
    for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True)[:20]:
        md += f"- **{lang}**ï¼š{count} ä¸ª\n"
    
    md += f"""
## âŒ é”™è¯¯åˆ†å¸ƒï¼ˆå¦‚æœ‰ï¼‰

"""
    error_dist = {}
    for r in results:
        st = r.get("status", "")
        if str(st).startswith("error"):
            error_dist[st] = error_dist.get(st, 0) + 1
    
    if error_dist:
        for err, cnt in sorted(error_dist.items(), key=lambda x: x[1], reverse=True):
            md += f"- **{err}**ï¼š{cnt} æ¬¡\n"
    else:
        md += "æ— é”™è¯¯\n"
    
    md += f"""
## ğŸ“ è§†é¢‘åˆ—è¡¨ï¼ˆå‰50æ¡ï¼‰

| è§†é¢‘ID | çŠ¶æ€ | å¯ç”¨è¯­è¨€ |
|--------|------|----------|
"""
    for r in results[:50]:
        vid = r.get("video_id", "")[:15]
        status = r.get("status", "")[:20]
        langs = ", ".join(r.get("all_langs", []))[:60]
        md += f"| {vid} | {status} | {langs} |\n"
    
    if total > 50:
        md += f"\nï¼ˆå…± {total} ä¸ªè§†é¢‘ï¼Œä»…å±•ç¤ºå‰ 50 æ¡ï¼‰\n"
    
    # ä¿å­˜
    preview_path = Path(run_dir) / "plan.md"
    preview_path.write_text(md, encoding="utf-8")
    return str(preview_path)

# ---------- è¿è¡Œè®°å½•ç®¡ç† ----------
def _run_dir(root: str) -> str:
    """åˆ›å»ºè¿è¡Œç›®å½•"""
    ts = time.strftime("%Y%m%d_%H%M%S")
    d = Path(root) / ts
    d.mkdir(parents=True, exist_ok=True)
    return str(d)

def _rec_path(run_dir: str) -> Path:
    """è·å–è®°å½•æ–‡ä»¶è·¯å¾„"""
    return Path(run_dir) / "run.jsonl"

def append_run_record(run_dir: str, rec: dict) -> None:
    """è¿½åŠ è¿è¡Œè®°å½•"""
    p = _rec_path(run_dir)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

def save_lists(results: List[Dict[str, Any]], run_dir: str) -> int:
    """ä¿å­˜ç»“æœåˆ—è¡¨æ–‡ä»¶"""
    d = Path(run_dir)
    allp = d / "all_links.txt"
    hasp = d / "has_subs.txt"
    nop = d / "no_subs.txt"
    with allp.open("w", encoding="utf-8") as fa, \
         hasp.open("w", encoding="utf-8") as fh, \
         nop.open("w", encoding="utf-8") as fn:
        for r in results:
            u = r["url"]
            st = r.get("status", "")
            fa.write(u + "\n")
            if st == "has_subs":
                fh.write(u + "\n")
            elif st == "no_subs":
                fn.write(u + "\n")
    errs = [r["url"] for r in results if str(r.get("status", "")).startswith("error")]
    (d / "errors.txt").write_text("\n".join(dict.fromkeys(errs)) + "\n" if errs else "", "utf-8")
    return len(errs)

# ---------- å­—å¹•éªŒè¯ ----------
def _count_effective_lines_txt(fp: Path) -> int:
    """ç»Ÿè®¡ TXT æœ‰æ•ˆè¡Œæ•°"""
    try:
        n = 0
        for ln in fp.read_text('utf-8', errors='ignore').splitlines():
            if ln.strip():
                n += 1
        return n
    except Exception:
        return 0

def validate_subtitles_dir(subs_dir: str, min_lines_txt: int = 5) -> list:
    """éªŒè¯å­—å¹•ç›®å½•ä¸­çš„æ–‡ä»¶"""
    out = []
    d = Path(subs_dir)
    if not d.exists():
        return out
    for fp in d.glob('*.*'):
        if not fp.is_file():
            continue
        ext = fp.suffix.lower()
        if ext not in ('.txt', '.srt', '.vtt'):
            continue
        if ext == '.txt':
            n = _count_effective_lines_txt(fp)
        else:
            # SRT/VTT ç®€åŒ–éªŒè¯
            n = _count_effective_lines_txt(fp)
        if n <= 0:
            out.append({"file": str(fp), "reason": "empty", "lines": n})
        elif n < max(1, int(min_lines_txt)):
            out.append({"file": str(fp), "reason": "too_short", "lines": n})
    return out

def write_warnings(run_dir: str, warnings: list) -> str:
    """å†™å…¥è­¦å‘Šæ–‡ä»¶"""
    if not warnings:
        return ""
    dst = Path(run_dir) / "warnings.txt"
    lines = []
    for w in warnings:
        lines.append(f"{w.get('reason','unknown')}\t{w.get('lines',0)}\t{w.get('file','')}")
    dst.write_text("\n".join(lines) + "\n", encoding='utf-8')
    return str(dst)

def _process_translations(run_dir: str, translate_config: dict, results: list[dict]) -> dict:
    """
    R2D4: å¤„ç†ç¿»è¯‘ä»»åŠ¡ï¼ˆåå¤„ç†é˜¶æ®µï¼‰
    
    Args:
        run_dir: è¿è¡Œç›®å½•
        translate_config: ç¿»è¯‘é…ç½® {"enabled": bool, "src": str, "tgt": str, "format": str, "provider": str}
        results: æ£€æµ‹ç»“æœåˆ—è¡¨
    
    Returns:
        {"translated": int, "provider": str, "files": list[str]}
    """
    if not translate_config or not translate_config.get("enabled", False):
        return {"translated": 0, "provider": "none", "files": []}
    
    from translator_bridge import translate_lines
    import re
    
    src = translate_config.get("src", "auto")
    tgt = translate_config.get("tgt", "en")
    fmt = translate_config.get("format", "srt")
    provider = translate_config.get("provider", "mock")
    
    subs_dir = Path(run_dir) / "subs"
    trans_dir = Path(run_dir) / "translations"
    trans_dir.mkdir(exist_ok=True, parents=True)
    
    translated_count = 0
    translated_files = []
    
    for r in results:
        if r.get("status") != "has_subs":
            continue
        
        vid = r.get("video_id", "")
        if not vid:
            continue
        
        # ç¡®å®šæºè¯­è¨€
        available_langs = (r.get("manual_langs") or []) + (r.get("auto_langs") or [])
        if not available_langs:
            continue
        
        # src="auto" æ—¶å–ç¬¬ä¸€ä¸ªå¯ç”¨è¯­è¨€
        if src == "auto":
            src_lang = str(available_langs[0]).lower()
        else:
            src_lang = src.lower()
        
        # æŸ¥æ‰¾æºå­—å¹•æ–‡ä»¶
        src_file = None
        for ext in [".txt", ".srt", ".vtt"]:
            # å°è¯•åŒ¹é…æ–‡ä»¶åæ ¼å¼ï¼švideo_id.lang.ext æˆ– video_id.ext
            candidates = list(subs_dir.glob(f"{vid}*.{src_lang}{ext}")) + list(subs_dir.glob(f"{vid}{ext}"))
            if candidates:
                src_file = candidates[0]
                break
        
        if not src_file or not src_file.exists():
            logging.info(f"[TRANSLATE] No source subtitle found for {vid} (lang={src_lang}), skipping")
            continue
        
        try:
            # è¯»å–æºå­—å¹•æ–‡æœ¬
            content = src_file.read_text(encoding="utf-8", errors="ignore")
            
            # æå–çº¯æ–‡æœ¬è¡Œï¼ˆç®€åŒ–å¤„ç†ï¼‰
            if src_file.suffix.lower() == ".srt":
                # SRT: è·³è¿‡åºå·å’Œæ—¶é—´è½´ï¼Œåªå–æ–‡æœ¬
                lines = []
                for line in content.splitlines():
                    line = line.strip()
                    if not line:
                        continue
                    # è·³è¿‡åºå·ï¼ˆçº¯æ•°å­—ï¼‰å’Œæ—¶é—´è½´ï¼ˆåŒ…å« -->ï¼‰
                    if re.match(r'^\d+$', line) or '-->' in line:
                        continue
                    lines.append(line)
            elif src_file.suffix.lower() == ".vtt":
                # VTT: è·³è¿‡ WEBVTT å¤´å’Œæ—¶é—´è½´
                lines = []
                for line in content.splitlines():
                    line = line.strip()
                    if not line or line.startswith('WEBVTT') or '-->' in line:
                        continue
                    lines.append(line)
            else:
                # TXT: æ¯è¡Œéƒ½æ˜¯å†…å®¹
                lines = [ln.strip() for ln in content.splitlines() if ln.strip()]
            
            if not lines:
                logging.info(f"[TRANSLATE] No text lines extracted from {src_file.name}, skipping")
                continue
            
            # ç¿»è¯‘
            translated_lines, meta = translate_lines(lines, src_lang, tgt, provider=provider)
            
            # å†™å…¥è¯‘æ–‡æ–‡ä»¶
            out_file = trans_dir / f"{vid}.{tgt}.{fmt}"
            
            if fmt == "txt":
                # TXT: æ¯è¡Œä¸€æ¡
                out_file.write_text("\n".join(translated_lines) + "\n", encoding="utf-8")
            elif fmt in ("srt", "vtt"):
                # SRT/VTT: ç®€åŒ–å®ç°ï¼ˆæ— æ—¶é—´è½´ï¼Œä»…æ–‡æœ¬ï¼‰
                # å®Œæ•´å®ç°éœ€è¦ä¿ç•™åŸå§‹æ—¶é—´è½´ï¼Œè¿™é‡Œå ä½
                out_file.write_text("\n".join(translated_lines) + "\n", encoding="utf-8")
            
            translated_count += 1
            translated_files.append(str(out_file.relative_to(run_dir)))
            
            logging.info(f"[TRANSLATE] Translated: {vid} -> {tgt} (provider={provider}, fmt={fmt}, lines={len(lines)})")
            
        except Exception as e:
            logging.warning(f"[TRANSLATE] Failed to translate {vid}: {e}")
            continue
    
    # ä¿å­˜ç¿»è¯‘å…ƒä¿¡æ¯
    meta_result = {
        "translated": translated_count,
        "provider": provider,
        "files": translated_files,
        "target_lang": tgt,
        "source_lang": src,
        "format": fmt
    }
    
    meta_file = trans_dir / "translation_meta.json"
    try:
        meta_file.write_text(json.dumps(meta_result, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass
    
    return meta_result


def diagnose_run(run_dir: str) -> str:
    """
    è¯Šæ–­è¿è¡Œç»“æœï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š
    
    è¿”å›è¯Šæ–­æ–‡æœ¬ï¼ŒåŒæ—¶å†™å…¥ diagnose.txt
    """
    total = 0
    n_has = 0
    n_no = 0
    n_err = 0
    e429 = 0
    e503 = 0
    etimeout = 0
    eprivate = 0
    egeo = 0
    other = 0
    zh_hits = 0
    en_hits = 0
    
    rec_path = _rec_path(run_dir)
    if rec_path.exists():
        for ln in rec_path.read_text('utf-8', errors='ignore').splitlines():
            try:
                r = json.loads(ln)
            except Exception:
                continue
            total += 1
            st = str(r.get('status', ''))
            if st == 'has_subs':
                n_has += 1
            elif st == 'no_subs':
                n_no += 1
            elif st.startswith('error'):
                n_err += 1
                if st == 'error_429':
                    e429 += 1
                elif st == 'error_503':
                    e503 += 1
                elif st == 'error_timeout':
                    etimeout += 1
                elif st == 'error_private':
                    eprivate += 1
                elif st == 'error_geo':
                    egeo += 1
                else:
                    other += 1
            for lc in (r.get('manual_langs') or []) + (r.get('auto_langs') or []):
                lc = (lc or '').lower()
                if lc == 'zh' or lc.startswith(('zh', 'cmn')):
                    zh_hits += 1
                elif lc == 'en' or lc.startswith('en'):
                    en_hits += 1
    
    warn_path = Path(run_dir) / "warnings.txt"
    warn_count = 0
    short_count = 0
    empty_count = 0
    if warn_path.exists():
        for ln in warn_path.read_text('utf-8', errors='ignore').splitlines():
            warn_count += 1
            seg = ln.split('\t')
            if seg:
                if seg[0] == 'too_short':
                    short_count += 1
                elif seg[0] == 'empty':
                    empty_count += 1
    
    pieces = []
    pieces.append(f"æ€»è®¡ {total} ä¸ªé“¾æ¥ï¼šâœ… æœ‰å­—å¹• {n_has}ï¼Œâ– æ— å­—å¹• {n_no}ï¼Œâ— é”™è¯¯ {n_err}ã€‚")
    if n_err and total:
        ratio = n_err / max(1, total)
        pieces.append(f"é”™è¯¯å æ¯”ï¼š{ratio:.1%}ï¼ˆ429={e429}, 503={e503}, timeout={etimeout}, private={eprivate}, geo={egeo}, å…¶å®ƒ={other}ï¼‰")
        if e429 > 0:
            pieces.append("å»ºè®®ï¼šé™ä½ req_rate æˆ–å¢å¤§ breaker_cooldown_secï¼›ä»£ç†è´¨é‡å·®æ—¶å¯åˆ‡æ¢ detect_mode='fast'ã€‚")
        if e503 > 0:
            pieces.append("å»ºè®®ï¼šæ”¾æ…¢å¹¶å‘æˆ–æ”¹ç”¨å¥åº·ä»£ç†ï¼›å¢å¤§æ‰¹å¤„ç†é—´éš”ï¼ˆbatch sleepï¼‰ã€‚")
        if etimeout > 0:
            pieces.append("å»ºè®®ï¼šå¢å¤§ timeout æˆ–æ£€æŸ¥ç½‘ç»œç¨³å®šæ€§ï¼›ä½¿ç”¨æ›´å¿«çš„ä»£ç†ã€‚")
        if ratio >= 0.3 and e429 == 0 and e503 == 0:
            pieces.append("å»ºè®®ï¼šæ£€æŸ¥ cookies/åœ°åŒº/ç§æœ‰è§†é¢‘ï¼›æˆ–å°† detect_mode='standard' æ”¹ä¸º 'fast' åšå¯¹æ¯”ã€‚")
    else:
        pieces.append("é”™è¯¯å æ¯”å¾ˆä½ï¼Œæ•´ä½“ç¨³å®šã€‚")
    
    if zh_hits == 0 and en_hits > 0:
        pieces.append("è§‚å¯Ÿï¼šè‹±æ–‡å­—å¹•è¾ƒå¤šè€Œä¸­æ–‡ä¸º 0ã€‚å¯åœ¨è®¾ç½®é‡Œå¯ç”¨è‡ªåŠ¨å­—å¹•ï¼ˆprefer='both'ï¼‰åŒæ—¶ä¿ç•™è‹±æ–‡å›é€€ã€‚")
    if warn_count > 0:
        pieces.append(f"å­—å¹•å†…å®¹å‘Šè­¦ {warn_count} æ¡ï¼ˆç©º={empty_count}ï¼Œè¿‡çŸ­={short_count}ï¼‰ã€‚å¯ä»¥æé«˜ min_lines_txt é˜ˆå€¼æˆ–äººå·¥æŠ½æŸ¥ã€‚")
    
    pieces.append("è‹¥ä¸ºå‡çº§æµ‹è¯•ï¼Œå¯å…ˆç”¨ dry_run / å°æ ·æœ¬æ‰¹æ¬¡è·‘ï¼Œç¡®è®¤ç¨³å®šåå†å…¨é‡ã€‚")
    
    diagnose_text = "\n".join(pieces)
    
    # å†™å…¥æ–‡ä»¶
    try:
        dst = Path(run_dir) / "diagnose.txt"
        dst.write_text(diagnose_text, encoding='utf-8')
    except Exception:
        pass
    
    return diagnose_text

def run_full_process(
    channel_or_playlist_url: str = "",
    output_root: str = "out",
    max_workers: int = 5,
    sleep_between: float = 0.5,
    retry_times: int = 2,
    max_items: int = 0,
    do_download: bool = False,
    download_langs: list[str] | None = None,
    download_prefer: str = "both",
    download_fmt: str = "srt",
    progress_callback: Callable[[int, int, str], None] | None = None,
    stop_event: threading.Event | None = None,
    pause_event: threading.Event | None = None,
    urls_override: list[str] | None = None,
    file_path: str | None = None,
    user_agent: str = "",
    proxy_text: str = "",
    proxy_mode: str = "round_robin",
    proxy_cool_down_sec: int = 300,
    proxy_max_fails: int = 2,
    proxy_blacklist_threshold: float = 0.15,
    proxy_window: int = 30,
    cookiefile: str = "",
    incremental_detect: bool = True,
    incremental_download: bool = True,
    force_refresh: bool = False,
    batch_size: int = 0,
    merge_bilingual: bool = False,
    detect_mode: str = "standard",
    adaptive_concurrency: bool = False,
    min_workers: int = 2,
    max_workers_cap: int = 20,
    early_stop_on_seen: bool = True,
    req_rate: float = 4.0,
    breaker_threshold: int = 8,
    breaker_cooldown_sec: float = 120.0,
    dry_run: bool = False,
    preferred_langs: list[str] | None = None,
    webhook_config: dict | None = None,
    translate_config: dict | None = None,
    postprocess_config: dict | None = None,
    quality_config: dict | None = None,
) -> dict:
    """
    ä¸»æµç¨‹ï¼šæ£€æµ‹ â†’ ä¸‹è½½ â†’ è®°å½• + Webhook é€šçŸ¥
    
    webhook_config: {url, timeout_sec, max_retry, secret, enable, events}
    
    Return:
    {"run_dir": str, "total": int, "downloaded": int, "errors": int, "last_seen": str|None}
    """
    Path(output_root).mkdir(parents=True, exist_ok=True)
    run_dir = _run_dir(output_root)
    
    # Webhook è¾…åŠ©å‡½æ•°ï¼ˆfire-and-forgetï¼‰
    def _fire_webhook(event: str, payload: dict):
        if not webhook_config or not webhook_config.get("enable"):
            return
        events_filter = webhook_config.get("events", [])
        if events_filter and event not in events_filter:
            return
        
        def _send():
            try:
                _send_webhook_with_retry(
                    webhook_config.get("url", ""),
                    {"event": event, "ts": _ts_utc(), **payload},
                    webhook_config.get("timeout_sec", 5),
                    webhook_config.get("max_retry", 3),
                    webhook_config.get("secret", "")
                )
            except Exception as e:
                try:
                    wf = Path(run_dir) / "warnings.txt"
                    with wf.open("a", encoding="utf-8") as f:
                        f.write(f"webhook_fail\t{event}\t{str(e)[:100]}\n")
                except:
                    pass
        
        threading.Thread(target=_send, daemon=True).start()
    
    # Webhook: run_start
    _fire_webhook("run_start", {
        "channel": channel_or_playlist_url,
        "output_root": output_root,
        "config": {
            "max_workers": max_workers,
            "do_download": do_download,
            "download_langs": download_langs or [],
            "dry_run": dry_run
        }
    })
    
    if progress_callback:
        progress_callback(-1, 0, "é˜¶æ®µï¼šå‡†å¤‡ç¯å¢ƒ/è§£æè¾“å…¥")
    
    # æ„å»ºä»£ç†æ± å’Œé™æµå™¨
    pool = build_proxy_pool(
        proxy_text, mode=proxy_mode, proxy_cool_down_sec=proxy_cool_down_sec,
        proxy_max_fails=proxy_max_fails, proxy_blacklist_threshold=proxy_blacklist_threshold,
        proxy_window=proxy_window
    )
    limiter = RateLimiter(req_rate, int(req_rate * 2)) if req_rate > 0 else None
    breaker = CircuitBreaker(breaker_threshold, breaker_cooldown_sec)
    
    # è·å–å¾…æ£€æµ‹çš„ URLsï¼ˆä¿ç•™æ‰€æœ‰URLï¼ŒåŒ…æ‹¬é‡å¤çš„ï¼Œä½†æ ‡å‡†åŒ–æ ¼å¼ï¼‰
    urls = []
    if urls_override:
        for u in urls_override:
            u_clean = u.strip()
            if u_clean:
                # æ ‡å‡†åŒ–URLæ ¼å¼
                u_normalized = normalize_url(u_clean)
                urls.append(u_normalized)
        
        # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        logger.debug(f"URLså¤„ç†: è¾“å…¥{len(urls_override)}ä¸ªï¼Œå¤„ç†{len(urls)}ä¸ª")
        if len(urls_override) != len(set(urls)):
            logger.debug("æ³¨æ„ï¼šè¾“å…¥ä¸­æœ‰é‡å¤URLï¼Œå°†åˆ†åˆ«å¤„ç†")
    elif file_path and os.path.exists(file_path):
        urls = [normalize_url(ln.strip()) for ln in open(file_path, "r", encoding="utf-8") if ln.strip()]
    elif channel_or_playlist_url:
        if progress_callback:
            progress_callback(-1, 0, "é˜¶æ®µï¼šæŠ“å–é¢‘é“/æ’­æ”¾åˆ—è¡¨")
        p = pool.get() if pool else ""
        urls = extract_all_video_urls_from_channel_or_playlist(
            channel_or_playlist_url, proxy=p, user_agent=user_agent, cookiefile=cookiefile
        )
    
    # å¢é‡ç­–ç•¥ï¼šè¯»å–ä¸Šæ¬¡ last_seen
    chan_idx = channel_index_load(output_root) if incremental_detect else {}
    last_seen = None
    chan_key = None
    if channel_or_playlist_url:
        chan_key = ensure_channel_videos_url(channel_or_playlist_url)
        last_seen = chan_idx.get(chan_key)
    
    # Dry æ¨¡å¼æç¤º
    if dry_run:
        logging.info(f"[DRY RUN] é¢„è§ˆæ¨¡å¼ï¼šä»…æ£€æµ‹ï¼Œä¸ä¸‹è½½/AIï¼Œå…± {len(urls)} ä¸ªURL")
    
    if progress_callback:
        progress_callback(-1, len(urls), f"å…± {len(urls)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹æ£€æµ‹å­—å¹•")
        if dry_run:
            progress_callback(-1, len(urls), "[DRY RUN] ä»…æ£€æµ‹ä¸ä¸‹è½½")
    
    # æ£€æµ‹å­—å¹•
    det_results = detect_links(
        urls, max_workers=max_workers, sleep_between=sleep_between, retry_times=retry_times,
        progress_callback=progress_callback, user_agent=user_agent, proxy_pool=pool,
        cookiefile=cookiefile, stop_event=stop_event, pause_event=pause_event,
        batch_size=batch_size, detect_mode=detect_mode, adaptive_concurrency=adaptive_concurrency,
        min_workers=min_workers, max_workers_cap=max_workers_cap, rate_limiter=limiter,
        circuit_breaker=breaker
    )
    
    # ç»Ÿä¸€å˜é‡åï¼Œå…œåº•ä¸ºåˆ—è¡¨ï¼ˆé˜²æ­¢ Noneï¼‰
    results = det_results or []
    if not isinstance(results, list):
        results = list(results)
    
    # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæ£€æµ‹ç»“æœ
    logger.debug(f"æ£€æµ‹å®Œæˆ: è¾“å…¥{len(urls)}ä¸ªURLï¼Œæ£€æµ‹ç»“æœ{len(results)}ä¸ª")
    if len(urls) != len(results):
        logger.warning("URLæ•°é‡å’Œæ£€æµ‹ç»“æœæ•°é‡ä¸ä¸€è‡´")
        logger.debug(f"  - è¾“å…¥URLs: {[normalize_url(u) for u in urls[:5]]}{'...' if len(urls) > 5 else ''}")
        logger.debug(f"  - æ£€æµ‹ç»“æœURLs: {[r.get('url', '') for r in results[:5]]}{'...' if len(results) > 5 else ''}")
        logger.debug(f"  - æ£€æµ‹ç»“æœæ•°é‡: {len(results)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤URLçš„æ£€æµ‹ç»“æœè¢«åˆå¹¶äº†
    result_urls = [r.get('url', '') for r in results]
    if len(result_urls) != len(set(result_urls)):
        logger.debug("å‘ç°é‡å¤URLåœ¨æ£€æµ‹ç»“æœä¸­ï¼Œå°†åˆ†åˆ«å¤„ç†æ¯ä¸ªç»“æœ")
    
    errs = save_lists(results, run_dir)
    
    newest = last_seen
    downloaded = 0
    skipped = 0
    failed = 0
    
    # è·Ÿè¸ªå·²ä¸‹è½½çš„video_idï¼Œé¿å…é‡å¤ä¸‹è½½ç›¸åŒè§†é¢‘
    downloaded_vids = set()
    
    # æ‰¹é‡é‡è¯•ï¼šè®°å½•å¤±è´¥çš„é¡¹
    failed_items = []  # ç”¨äºæ‰¹é‡é‡è¯•å¤±è´¥çš„é¡¹ç›®
    
    # è¿›åº¦ç»Ÿè®¡
    start_time = time.perf_counter()
    last_progress_time = start_time
    
    # æ‰¹é‡æ“ä½œçŠ¶æ€è·Ÿè¸ª
    current_item_index = 0  # å½“å‰å¤„ç†çš„é¡¹ç´¢å¼•
    current_item_id = ""    # å½“å‰å¤„ç†çš„è§†é¢‘ID
    current_item_start_time = None  # å½“å‰é¡¹å¼€å§‹æ—¶é—´
    
    def _send_progress_update(current: int, total: int, message: str, stats: dict = None, current_item: str = None):
        """å‘é€è¿›åº¦æ›´æ–°ï¼ˆå¸¦ç»Ÿè®¡ä¿¡æ¯å’Œå½“å‰é¡¹ä¿¡æ¯ï¼‰"""
        if progress_callback:
            # å¦‚æœ message æ˜¯ dictï¼Œè¯´æ˜å·²ç»æ˜¯æ ¼å¼åŒ–çš„è¿›åº¦æ•°æ®
            if isinstance(message, dict):
                progress_data = message
            else:
                # å¦åˆ™æ„å»ºè¿›åº¦æ•°æ®
                progress_data = {
                    "current": current,
                    "total": total,
                    "message": message,
                    "percentage": (current / total * 100) if total > 0 else 0
                }
                if stats:
                    progress_data.update(stats)
            
            # æ·»åŠ å½“å‰é¡¹ä¿¡æ¯
            if current_item:
                progress_data["current_item"] = current_item
                progress_data["current_item_index"] = current_item_index + 1
            
            # è®¡ç®—å½“å‰é¡¹è€—æ—¶
            if current_item_start_time:
                progress_data["current_item_elapsed"] = time.perf_counter() - current_item_start_time
            
            # é€‚é…å™¨ä¼šå°† dict è½¬æ¢ä¸º (current, total, message) æ ¼å¼
            progress_callback(-1, total, progress_data)
    
    for idx, r in enumerate(results):
        # æ£€æŸ¥åœæ­¢äº‹ä»¶
        if stop_event and stop_event.is_set():
            if progress_callback:
                _send_progress_update(
                    downloaded + skipped + failed, len(results),
                    f"â¹ï¸ ç”¨æˆ·åœæ­¢æ“ä½œ",
                    {
                        "downloaded": downloaded,
                        "skipped": skipped,
                        "failed": failed,
                        "phase": "stopped"
                    }
                )
            break
        
        # æ£€æŸ¥æš‚åœäº‹ä»¶
        while pause_event and pause_event.is_set():
            if stop_event and stop_event.is_set():
                break
            time.sleep(0.1)
            if progress_callback and int(time.time()) % 2 == 0:  # æ¯2ç§’æ˜¾ç¤ºä¸€æ¬¡æš‚åœçŠ¶æ€
                _send_progress_update(
                    downloaded + skipped + failed, len(results),
                    f"â¸ï¸ å·²æš‚åœ... (æŒ‰ç»§ç»­æŒ‰é’®æ¢å¤)",
                    {
                        "downloaded": downloaded,
                        "skipped": skipped,
                        "failed": failed,
                        "phase": "paused"
                    }
                )
        
        current_item_index = idx
        current_item_id = r.get("video_id", "")
        current_item_start_time = time.perf_counter()
        
        meta = r.get("meta") or {}
        # ä¿®å¤ï¼šupload_dateå¯èƒ½åœ¨é¡¶å±‚æˆ–metaä¸­
        up = r.get("upload_date") or meta.get("upload_date")  # "YYYYMMDD"
        vid = r.get("video_id")
        is_new = (not last_seen) or (up and up > last_seen)
        
        # è¯­è¨€é€‰æ‹©é€»è¾‘ï¼šä¼˜å…ˆåŒ¹é…æ‰€æœ‰selected_langsä¸­åœ¨available_langsé‡Œçš„è¯­è¨€
        available_langs = (r.get("manual_langs") or []) + (r.get("auto_langs") or [])
        selected_langs = download_langs or ["zh", "en"]
        final_langs = [x.lower() for x in selected_langs]
        fallback_reason = None
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹æ£€æµ‹ç»“æœ
        logger.debug(f"Video {vid} language detection:")
        logger.debug(f"  - manual_langs (raw): {r.get('manual_langs')}")
        logger.debug(f"  - auto_langs (raw): {r.get('auto_langs')}")
        logger.debug(f"  - available_langs (combined): {available_langs}")
        logger.debug(f"  - selected_langs (from config): {selected_langs}")
        
        # è¯­è¨€é€‰æ‹©é€»è¾‘ï¼šä¼˜å…ˆåŒ¹é…æ‰€æœ‰selected_langsä¸­åœ¨available_langsé‡Œçš„è¯­è¨€
        # å¦‚æœpreferred_langsä¸ºNoneï¼Œç›´æ¥ä½¿ç”¨selected_langsåŒ¹é…
        tmp = []  # åˆå§‹åŒ–tmpå˜é‡
        if preferred_langs is not None:
            seen = set()
            tmp = []
            # é¦–å…ˆæ·»åŠ  preferred_langs ä¸­åœ¨ available_langs ä¸­çš„è¯­è¨€
            for pl in preferred_langs:
                pl_l = (pl or "").lower()
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨è¯­è¨€åŒ¹é…ï¼ˆæ”¯æŒå‰ç¼€åŒ¹é…ï¼Œå¦‚ zh-Hans åŒ¹é… zhï¼‰
                matched = False
                for avail_lang in available_langs:
                    avail_l = (avail_lang or "").lower()
                    if avail_l.startswith(pl_l) or pl_l.startswith(avail_l.split('-')[0]):
                        matched = True
                        break
                if matched and pl_l not in seen:
                    tmp.append(pl_l)
                    seen.add(pl_l)
            
            # å¦‚æœ preferred_langs åŒ¹é…åˆ°äº†è¯­è¨€ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™å°è¯• fallback
            if tmp:
                final_langs = tmp
            else:
                # Fallback: ä½¿ç”¨æ‰€æœ‰åœ¨ available_langs ä¸­çš„ selected_langs
                tmp = []
                for sl in selected_langs:
                    sl_l = (sl or "").lower()
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨è¯­è¨€åŒ¹é…
                    for avail_lang in available_langs:
                        avail_l = (avail_lang or "").lower()
                        if avail_l.startswith(sl_l) or sl_l.startswith(avail_l.split('-')[0]):
                            if sl_l not in seen:
                                tmp.append(sl_l)
                                seen.add(sl_l)
                            break
                
                if tmp:
                    final_langs = tmp
                elif available_langs:
                    # æœ€åçš„ fallback: ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è¯­è¨€
                    final_langs = [str(available_langs[0]).lower()]
                    fallback_reason = f"fallback\t{','.join(selected_langs)}-> {final_langs[0]}\t{vid}"
        else:
            # å¦‚æœæ²¡æœ‰ preferred_langsï¼Œç›´æ¥åŒ¹é…æ‰€æœ‰ selected_langs ä¸­åœ¨ available_langs é‡Œçš„è¯­è¨€
            tmp = []
            seen = set()
            for sl in selected_langs:
                sl_l = (sl or "").lower()
                # éå†æ‰€æœ‰å¯ç”¨è¯­è¨€ï¼Œæ‰¾åˆ°åŒ¹é…çš„å°±æ·»åŠ ï¼ˆä¸è¦breakï¼Œç¡®ä¿åŒ¹é…æ‰€æœ‰è¯­è¨€ï¼‰
                matched = False
                for avail_lang in available_langs:
                    avail_l = (avail_lang or "").lower()
                    # æ”¯æŒå‰ç¼€åŒ¹é…ï¼ˆzh-Hans åŒ¹é… zhï¼Œæˆ– zh åŒ¹é… zh-Hansï¼‰
                    if avail_l.startswith(sl_l) or sl_l.startswith(avail_l.split('-')[0]):
                        if sl_l not in seen:
                            tmp.append(sl_l)
                            seen.add(sl_l)
                            matched = True
                            break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±é€€å‡ºå†…å±‚å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª selected_lang
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè®°å½•æ—¥å¿—ï¼ˆä½†ä¸é˜»æ­¢å¤„ç†ï¼‰
                if not matched:
                    logger.debug(f"è¯­è¨€ {sl_l} åœ¨å¯ç”¨è¯­è¨€ {list(available_langs)} ä¸­æœªæ‰¾åˆ°åŒ¹é…")
            
            if tmp:
                final_langs = tmp
            elif available_langs:
                # Fallback: ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è¯­è¨€
                final_langs = [str(available_langs[0]).lower()]
                fallback_reason = f"fallback\t{','.join(selected_langs)}-> {final_langs[0]}\t{vid}"
        
        # é‡è¦ä¿®å¤ï¼šå³ä½¿æ£€æµ‹æ—¶æ²¡æœ‰æ£€æµ‹åˆ°æ‰€æœ‰è¯­è¨€ï¼Œä¹Ÿå°è¯•ä¸‹è½½æ‰€æœ‰é…ç½®çš„è¯­è¨€
        # å› ä¸º yt-dlp åœ¨å®é™…ä¸‹è½½æ—¶å¯èƒ½ä¼šè¿”å›æ›´å¤šå¯ç”¨è¯­è¨€
        if tmp and len(tmp) < len(selected_langs):
            # å¦‚æœæ£€æµ‹åˆ°äº†ä¸€äº›è¯­è¨€ä½†å°‘äºé…ç½®çš„è¯­è¨€ï¼Œå°è¯•è¡¥å……ï¼ˆè®©yt-dlpå°è¯•ï¼‰
            # æ·»åŠ æœªåŒ¹é…åˆ°çš„é…ç½®è¯­è¨€
            missing_langs = [sl.lower() for sl in selected_langs if sl.lower() not in tmp]
            if missing_langs:
                logger.debug(f"æ£€æµ‹åˆ°éƒ¨åˆ†è¯­è¨€ {tmp}ï¼Œå°†å°è¯•è¡¥å……: {missing_langs}")
                final_langs = tmp + missing_langs  # åˆå¹¶å·²åŒ¹é…å’ŒæœªåŒ¹é…çš„è¯­è¨€
        elif not tmp and not available_langs:
            # å¦‚æœå®Œå…¨æ²¡æœ‰æ£€æµ‹åˆ°è¯­è¨€ï¼Œå°è¯•æ‰€æœ‰é…ç½®çš„è¯­è¨€ï¼ˆè®©yt-dlpå°è¯•ï¼‰
            final_langs = [x.lower() for x in selected_langs]
            logger.debug(f"æœªæ£€æµ‹åˆ°ä»»ä½•è¯­è¨€ï¼Œå°†å°è¯•ä¸‹è½½æ‰€æœ‰é…ç½®çš„è¯­è¨€: {final_langs}")
        
        final_langs = [x for i, x in enumerate(final_langs) if x and x not in final_langs[:i]]
        
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°è¯­è¨€é€‰æ‹©ç»“æœ
        logger.debug(f"Video {vid} language selection:")
        logger.debug(f"  - selected_langs (from config): {selected_langs}")
        logger.debug(f"  - available_langs: {list(available_langs)}")
        logger.debug(f"  - preferred_langs: {preferred_langs}")
        logger.debug(f"  - final_langs (will download): {final_langs}")
        if fallback_reason:
            logger.debug(f"  - fallback_reason: {fallback_reason}")
        
        # è®°å½•è¿è¡Œæ—¥å¿—ï¼ˆæ£€æµ‹é˜¶æ®µï¼‰
        append_run_record(run_dir, {
            "action": "detect",  # åŠ¨ä½œæ ‡è¯†ï¼šæ£€æµ‹
            "ts": _ts_utc(),
            "url": r["url"],
            "video_id": vid,
            "status": r.get("status"),
            "manual_langs": r.get("manual_langs", []),
            "auto_langs": r.get("auto_langs", []),
            "proxy": "",
            "latency_ms": r.get("latency_ms"),
            "attempts": r.get("attempts"),
            "detector": "yta+ydlp" if detect_mode == "standard" else "ytdlp",
            "err": r.get("api_err"),
            "title": meta.get("title"),
            "channel": meta.get("channel") or meta.get("uploader"),
            "upload_date": up,
            "duration": meta.get("duration"),
            "view_count": meta.get("view_count"),
            "tags": meta.get("tags") or [],
            "final_langs": final_langs
        })
        history_append(output_root, {"video_id": vid, "upload_date": up, "ts": _ts_utc()})
        
        # Step 3 Day1: å†™å…¥ç»Ÿä¸€å†å²è®°å½•
        # Day2: ä½¿ç”¨å¢å¼ºçš„é”™è¯¯åˆ†ç±»
        if _HISTORY_AVAILABLE:
            try:
                has_subs = r.get("status") == "has_subs"
                error_msg = r.get("api_err") if r.get("status", "").startswith("error") else None
                
                # Day2: ä½¿ç”¨ v2 ç‰ˆæœ¬è·å–å®Œæ•´é”™è¯¯ä¿¡æ¯
                status, error_code, error_msg_simplified, error_class, retryable = classify_status_v2(error_msg, has_subs)
                
                history_row: HistoryRow = {
                    "video_id": vid or "",
                    "url": r.get("url", ""),
                    "title": meta.get("title", ""),
                    "channel": meta.get("channel") or meta.get("uploader") or "",
                    "status": status,
                    "error_code": error_code,
                    "error_msg": error_msg_simplified,
                    "error_class": error_class,
                    "retryable": retryable,
                    "langs": available_langs,
                    "upload_date": up or "",
                    "duration": meta.get("duration", 0) or 0,
                    "view_count": meta.get("view_count", 0) or 0,
                }
                write_history(run_dir, history_row)
            except Exception as e:
                logging.warning(f"å†™å…¥å†å²è®°å½•å¤±è´¥: {e}")
        
        if up and (not newest or up > newest):
            newest = up
        
        # ä¸‹è½½å­—å¹•ï¼ˆdry æ¨¡å¼è·³è¿‡ï¼‰
        # DEBUG: æ‰“å°ä¸‹è½½æ¡ä»¶åˆ¤æ–­
        logger.debug(f"Video {vid} download check:")
        logger.debug(f"  - dry_run: {dry_run}")
        logger.debug(f"  - do_download: {do_download}")
        logger.debug(f"  - status: {r.get('status')}")
        logger.debug(f"  - is_new: {is_new}")
        logger.debug(f"  - last_seen: {last_seen}")
        logger.debug(f"  - upload_date: {up}")
        logger.debug(f"  - force_refresh: {force_refresh}")
        # åŸºç¡€æ¡ä»¶ï¼šédry_runã€å¼€å¯ä¸‹è½½
        # å¦‚æœæ£€æµ‹åˆ°æœ‰å­—å¹•ï¼Œæˆ–è€…ç”¨æˆ·å¼ºåˆ¶åˆ·æ–°/æ˜ç¡®é…ç½®äº†ä¸‹è½½è¯­è¨€ï¼Œéƒ½åº”è¯¥å°è¯•ä¸‹è½½
        # å› ä¸º yt-dlp åœ¨å®é™…ä¸‹è½½æ—¶å¯èƒ½ä¼šæ£€æµ‹åˆ°å­—å¹•ï¼ˆå³ä½¿æ£€æµ‹é˜¶æ®µæ²¡æœ‰æ£€æµ‹åˆ°ï¼‰
        has_detected_subs = r.get("status") == "has_subs"
        user_requested_langs = bool(final_langs)  # ç”¨æˆ·æ˜ç¡®é…ç½®äº†è¦ä¸‹è½½çš„è¯­è¨€
        should_try_download = has_detected_subs or force_refresh or user_requested_langs
        
        will_download = (not dry_run) and do_download and should_try_download
        # å¼ºåˆ¶åˆ·æ–°æ—¶å¿½ç•¥is_newåˆ¤æ–­ï¼Œå¦åˆ™éœ€è¦æ£€æŸ¥is_new
        if not force_refresh:
            will_download = will_download and is_new
        logger.debug(f"  - Will download: {will_download} (has_subs={has_detected_subs}, force_refresh={force_refresh}, user_langs={user_requested_langs})")
        
        if will_download:
            # å¦‚æœforce_refresh=Falseï¼Œæ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½è¿‡ç›¸åŒvideo_id
            if not force_refresh and vid in downloaded_vids:
                logger.debug(f"è·³è¿‡é‡å¤è§†é¢‘ {vid}ï¼ˆå·²ä¸‹è½½ï¼‰")
                skipped += 1
                # å‘é€è¿›åº¦æ›´æ–°
                current_progress = downloaded + skipped + failed
                if progress_callback and current_progress % 5 == 0:  # æ¯5ä¸ªæ›´æ–°ä¸€æ¬¡
                    elapsed = time.perf_counter() - start_time
                    speed = current_progress / elapsed if elapsed > 0 else 0
                    remaining = (len(results) - current_progress) / speed if speed > 0 else 0
                    _send_progress_update(
                        current_progress, len(results),
                        f"å¤„ç†ä¸­: {current_progress}/{len(results)} (âœ“{downloaded} âš {skipped} âœ—{failed})",
                        {
                            "downloaded": downloaded,
                            "skipped": skipped,
                            "failed": failed,
                            "speed": speed,
                            "remaining": remaining
                        }
                    )
                continue
            
            logger.debug(f"Starting download for {vid}...")
            
            # é€šè¿‡progress_callbackå‘é€ä¸‹è½½å¼€å§‹æ¶ˆæ¯ï¼ˆå¸¦å½“å‰é¡¹ä¿¡æ¯ï¼‰
            if progress_callback:
                _send_progress_update(
                    downloaded + skipped + failed, len(results),
                    f"å¼€å§‹ä¸‹è½½: {vid} ({', '.join(final_langs)})",
                    {
                        "downloaded": downloaded,
                        "skipped": skipped,
                        "failed": failed,
                        "phase": "downloading"
                    },
                    current_item=vid
                )
            
            t0_dl = time.perf_counter()
            paths = download_subtitles(
                r["url"], str(Path(run_dir) / "subs"), final_langs, download_prefer, download_fmt,
                user_agent=user_agent, proxy_pool=pool, cookiefile=cookiefile,
                stop_event=stop_event, pause_event=pause_event, retry_times=retry_times,
                base_sleep=sleep_between, incremental=incremental_download,
                merge_bilingual=merge_bilingual, rate_limiter=limiter, circuit_breaker=breaker
            )
            downloaded += int(bool(paths))
            
            # æ›´æ–°ç»Ÿè®¡
            if not paths:
                failed += 1
                # è®°å½•å¤±è´¥é¡¹ï¼Œç”¨äºæ‰¹é‡é‡è¯•
                failed_items.append({
                    "video_id": vid,
                    "url": r["url"],
                    "status": r.get("status"),
                    "available_langs": available_langs,
                    "final_langs": final_langs,
                    "error": "download_failed"
                })
            
            # å‘é€ä¸‹è½½å®Œæˆæ¶ˆæ¯ï¼ˆå¸¦ç»Ÿè®¡ï¼‰
            current_progress = downloaded + skipped + failed
            elapsed = time.perf_counter() - start_time
            speed = current_progress / elapsed if elapsed > 0 else 0
            remaining = (len(results) - current_progress) / speed if speed > 0 else 0
            
            if progress_callback:
                if paths:
                    _send_progress_update(
                        current_progress, len(results),
                        f"âœ… {vid}: ä¸‹è½½æˆåŠŸ ({len(paths)} ä¸ªæ–‡ä»¶)",
                        {
                            "downloaded": downloaded,
                            "skipped": skipped,
                            "failed": failed,
                            "speed": speed,
                            "remaining": remaining,
                            "phase": "download_complete"
                        },
                        current_item=vid
                    )
                else:
                    _send_progress_update(
                        current_progress, len(results),
                        f"âš ï¸ {vid}: ä¸‹è½½å¤±è´¥",
                        {
                            "downloaded": downloaded,
                            "skipped": skipped,
                            "failed": failed,
                            "speed": speed,
                            "remaining": remaining,
                            "phase": "download_failed"
                        },
                        current_item=vid
                    )
            
            # è®°å½•ä¸‹è½½åŠ¨ä½œ
            if paths:
                downloaded_vids.add(vid)  # è®°å½•å·²ä¸‹è½½çš„video_id
                
                # å­—å¹•ä¼˜åŒ–ï¼ˆä¸‹è½½å®Œæˆåï¼‰
                optimize_result = None
                if (postprocess_config and postprocess_config.get("enabled", False)) or \
                   (quality_config and quality_config.get("enabled", False)):
                    try:
                        from services.subtitle_optimize_service import SubtitleOptimizeService
                        
                        optimize_config = {
                            "postprocess": postprocess_config or {},
                            "quality": quality_config or {}
                        }
                        optimize_service = SubtitleOptimizeService(optimize_config)
                        
                        # æ‰¹é‡ä¼˜åŒ–ä¸‹è½½çš„å­—å¹•æ–‡ä»¶
                        optimize_result = optimize_service.optimize_subtitle_files(paths)
                        
                        if optimize_result.get("optimized", 0) > 0:
                            logging.info(
                                f"[OPTIMIZE] {vid}: ä¼˜åŒ–äº† {optimize_result['optimized']}/{optimize_result['total']} ä¸ªå­—å¹•æ–‡ä»¶"
                            )
                    except Exception as e:
                        logging.warning(f"[OPTIMIZE] å­—å¹•ä¼˜åŒ–å¤±è´¥: {e}")
                        optimize_result = {"error": str(e)}
                
                append_run_record(run_dir, {
                    "action": "download",  # åŠ¨ä½œæ ‡è¯†ï¼šä¸‹è½½
                    "ts": _ts_utc(),
                    "url": r["url"],
                    "video_id": vid,
                    "status": "success",
                    "files": [str(p) for p in paths],
                    "langs": final_langs,
                    "format": download_fmt,
                    "latency_ms": (time.perf_counter() - t0_dl) * 1000.0,
                    "optimize": optimize_result  # æ·»åŠ ä¼˜åŒ–ç»“æœ
                })
            
            if fallback_reason:
                wf = Path(run_dir) / "warnings.txt"
                try:
                    with wf.open("a", encoding="utf-8") as wfo:
                        wfo.write(f"{fallback_reason}\n")
                except Exception:
                    pass
    
    # B1: ASR æ— å­—å¹•è¡¥å…¨ï¼ˆæ£€æµ‹/ä¸‹è½½åï¼‰
    asr_result = {"completed": 0, "provider": "none", "files": []}
    asr_config = translate_config.get("asr", {}) if translate_config else {}
    
    # ä»é…ç½®åŠ è½½ ASR è®¾ç½®
    if not asr_config:
        try:
            from .config import load_config
            cfg = load_config()
            asr_config = cfg.get("asr", {})
        except Exception:
            pass
    
    if not dry_run and asr_config.get("enabled", False):
        try:
            from .asr_bridge import run_asr
            
            if progress_callback:
                progress_callback(-1, len(urls), "é˜¶æ®µï¼šASR è¡¥å…¨")
            
            asr_dir = Path(run_dir) / "asr"
            asr_dir.mkdir(exist_ok=True, parents=True)
            
            for r in results:
                # åªå¯¹æ— å­—å¹•æˆ–æ— ç›®æ ‡è¯­çš„è§†é¢‘æ‰§è¡Œ ASR
                if r.get("status") in ("no_subs",) or not r.get("manual_langs", []):
                    try:
                        asr_res = run_asr(
                            video_url=r.get("url", ""),
                            provider=asr_config.get("provider", "mock"),
                            lang_hint=asr_config.get("lang_hint", "auto"),
                            out_dir=str(asr_dir),
                            timeout=asr_config.get("timeout", 120)
                        )
                        
                        if asr_res.get("success"):
                            # æ ‡æ³¨è§†é¢‘å·²æœ‰å­—å¹•ï¼ˆASR ç”Ÿæˆï¼‰
                            r["status"] = "has_subs"
                            r.setdefault("manual_langs", []).append(asr_res.get("lang", ""))
                            asr_result["completed"] += 1
                            asr_result["files"].append(asr_res.get("file", ""))
                            asr_result["provider"] = asr_config.get("provider", "mock")
                            
                            logging.info(f"[ASR-B1] å®Œæˆ: {r.get('video_id')} -> {asr_res.get('file')}")
                    
                    except Exception as e:
                        logging.warning(f"[ASR-B1] å¤±è´¥ {r.get('video_id')}: {e}")
            
            if asr_result["completed"] > 0:
                logging.info(f"[ASR-B1] è¡¥å…¨å®Œæˆ: {asr_result['completed']} ä¸ªè§†é¢‘ (provider={asr_result['provider']})")
        
        except Exception as e:
            logging.warning(f"[ASR-B1] ASR è¡¥å…¨å¤±è´¥: {e}")
    
    # éªŒè¯å­—å¹•æ–‡ä»¶ï¼ˆdry æ¨¡å¼è·³è¿‡ï¼‰
    warnings = []
    if not dry_run:
        warnings = validate_subtitles_dir(str(Path(run_dir) / "subs"), min_lines_txt=5)
        write_warnings(run_dir, warnings)
    else:
        logging.info(f"[DRY RUN] è·³è¿‡å­—å¹•éªŒè¯")
    
    # R2D4: ç¿»è¯‘å¤„ç†ï¼ˆåå¤„ç†é˜¶æ®µï¼‰
    translation_result = {"translated": 0, "provider": "none", "files": [], "target_lang": ""}
    if not dry_run and translate_config:
        try:
            if progress_callback:
                progress_callback(-1, len(urls), "é˜¶æ®µï¼šç¿»è¯‘å­—å¹•")
            translation_result = _process_translations(run_dir, translate_config, results)
            if translation_result.get("translated", 0) > 0:
                logging.info(f"[TRANSLATE] å®Œæˆç¿»è¯‘ï¼š{translation_result['translated']} ä¸ªè§†é¢‘ -> {translation_result.get('target_lang', 'unknown')}")
        except Exception as e:
            logging.warning(f"[TRANSLATE] ç¿»è¯‘å¤„ç†å¤±è´¥: {e}")
    
    # A2: ä¸­æ–‡æ¸…æ´—ä¸æœ¯è¯­ç»Ÿä¸€ï¼ˆç¿»è¯‘åå¤„ç†ï¼‰
    cleanup_result = {"cleaned": 0, "terminology_replaced": 0, "lines_merged": 0}
    if not dry_run and translate_config and translate_config.get("postprocess", True):
        try:
            from .cleanup_zh import clean_subtitle_file, load_terminology
            
            if progress_callback:
                progress_callback(-1, len(urls), "é˜¶æ®µï¼šæ¸…æ´—ä¸æœ¯è¯­ç»Ÿä¸€")
            
            # åŠ è½½æœ¯è¯­è¡¨
            terminology = load_terminology("terminology.json")
            
            # å¤„ç†ç¿»è¯‘åçš„æ–‡ä»¶
            trans_dir = Path(run_dir) / "translations"
            if trans_dir.exists():
                for trans_file in trans_dir.glob("*.txt"):
                    try:
                        stats = clean_subtitle_file(
                            input_file=str(trans_file),
                            output_file=None,  # è¦†ç›–åŸæ–‡ä»¶
                            terminology_file="terminology.json",
                            merge_enabled=True
                        )
                        cleanup_result["cleaned"] += 1
                        cleanup_result["terminology_replaced"] += stats.get("terminology_replaced", 0)
                        cleanup_result["lines_merged"] += stats.get("lines_merged", 0)
                    except Exception as e:
                        logging.warning(f"[CLEANUP-A2] æ¸…æ´—å¤±è´¥ {trans_file.name}: {e}")
            
            # ä¹Ÿå¤„ç† ASR ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆå¦‚æœæ˜¯ä¸­æ–‡ï¼‰
            asr_dir = Path(run_dir) / "asr"
            if asr_dir.exists():
                for asr_file in asr_dir.glob("*.txt"):
                    try:
                        stats = clean_subtitle_file(
                            input_file=str(asr_file),
                            output_file=None,
                            terminology_file="terminology.json",
                            merge_enabled=True
                        )
                        cleanup_result["cleaned"] += 1
                        cleanup_result["terminology_replaced"] += stats.get("terminology_replaced", 0)
                        cleanup_result["lines_merged"] += stats.get("lines_merged", 0)
                    except Exception as e:
                        logging.warning(f"[CLEANUP-A2] æ¸…æ´—ASRæ–‡ä»¶å¤±è´¥ {asr_file.name}: {e}")
            
            if cleanup_result["cleaned"] > 0:
                logging.info(
                    f"[CLEANUP-A2] å®Œæˆæ¸…æ´—: {cleanup_result['cleaned']} ä¸ªæ–‡ä»¶ "
                    f"(æœ¯è¯­={cleanup_result['terminology_replaced']}, åˆå¹¶={cleanup_result['lines_merged']})"
                )
        
        except Exception as e:
            logging.warning(f"[CLEANUP-A2] æ¸…æ´—å¤„ç†å¤±è´¥: {e}")
    
    # CDçº¿: åŒè¯­å­—å¹•åˆå¹¶ï¼ˆåå¤„ç†é˜¶æ®µï¼‰
    bilingual_result = {"total": 0, "success": 0, "files": [], "format": ""}
    if not dry_run and merge_bilingual:
        try:
            from .exports import export_bilingual_subtitles
            from .config import load_config
            import sys
            
            if progress_callback:
                progress_callback(-1, len(urls), "é˜¶æ®µï¼šç”ŸæˆåŒè¯­å­—å¹•")
            
            # ä»é…ç½®è¯»å–åŒè¯­è®¾ç½®
            cfg = load_config()
            bi_cfg = cfg.get("bilingual", {})
            
            logger.info("="*60)
            logger.info("[BILINGUAL] å¼€å§‹åŒè¯­å­—å¹•åˆå¹¶...")
            logger.info(f"  - ä¸»è¯­è¨€: {bi_cfg.get('primary', 'auto')}")
            logger.info(f"  - æ¬¡è¯­è¨€: {bi_cfg.get('secondary', 'en')}")
            logger.info(f"  - è¾“å‡ºæ ¼å¼: {bi_cfg.get('format', 'tsv')}")
            logger.info(f"  - è¾“å‡ºç›®å½•: {bi_cfg.get('output_dir', 'bilingual')}")
            logger.info("="*60)
            
            bilingual_result = export_bilingual_subtitles(
                run_dir=run_dir,
                primary_lang=bi_cfg.get("primary", "auto"),
                secondary_lang=bi_cfg.get("secondary", "en"),
                output_format=bi_cfg.get("format", "tsv"),
                output_subdir=bi_cfg.get("output_dir", "bilingual")
            )
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            total = bilingual_result.get("total", 0)
            success = bilingual_result.get("success", 0)
            failed = total - success
            format_type = bilingual_result.get("format", "tsv")
            files = bilingual_result.get("files", [])
            
            logger.info("="*60)
            logger.info("[BILINGUAL] åŒè¯­åˆå¹¶å®Œæˆ")
            logger.info(f"  - æ€»è®¡: {total} ä¸ªè§†é¢‘")
            logger.info(f"  - æˆåŠŸ: {success} ä¸ª")
            if failed > 0:
                logger.info(f"  - è·³è¿‡: {failed} ä¸ªï¼ˆå¯èƒ½ç¼ºå°‘åŒè¯­å­—å¹•ï¼‰")
            logger.info(f"  - æ ¼å¼: {format_type.upper()}")
            if files:
                logger.info("  - ç”Ÿæˆæ–‡ä»¶:")
                for f in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    logger.info(f"    â€¢ {f}")
                if len(files) > 5:
                    logger.info(f"    ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
            logger.info("="*60)
            
            if success > 0:
                logging.info(f"[BILINGUAL] å®ŒæˆåŒè¯­åˆå¹¶ï¼š{success}/{total} ä¸ªè§†é¢‘ (æ ¼å¼={format_type})")
            elif total > 0:
                logging.warning(f"[BILINGUAL] åŒè¯­åˆå¹¶ï¼š{total} ä¸ªè§†é¢‘å…¨éƒ¨è·³è¿‡ï¼ˆå¯èƒ½ç¼ºå°‘åŒè¯­å­—å¹•ï¼‰")
        except Exception as e:
            logger.error(f"[BILINGUAL] åŒè¯­åˆå¹¶å¤±è´¥: {e}", exc_info=True)
    
    # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š / dry æ¨¡å¼ç”Ÿæˆé¢„è§ˆ
    try:
        if not dry_run:
            diagnose_text = diagnose_run(run_dir)
        else:
            # Dry æ¨¡å¼ç”Ÿæˆé¢„è§ˆæŠ¥å‘Š
            _generate_dry_preview(run_dir, results)
            logging.info(f"[DRY RUN] é¢„è§ˆæŠ¥å‘Šå·²ç”Ÿæˆï¼š{run_dir}/plan.md")
    except Exception as e:
        logging.warning(f"è¯Šæ–­/é¢„è§ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    # æ›´æ–°é¢‘é“ç´¢å¼•ï¼ˆdry æ¨¡å¼ä¸æ›´æ–°ï¼Œé¿å…æ±¡æŸ“ï¼‰
    if not dry_run and chan_key and newest and (not last_seen or newest > last_seen):
        chan_idx[chan_key] = newest
        channel_index_save(output_root, chan_idx)
    
    # ä¿å­˜é…ç½®å¿«ç…§
    from .config import save_config_snapshot
    try:
        final_config = {
            "output_root": output_root,
            "max_workers": max_workers,
            "sleep_between": sleep_between,
            "retry_times": retry_times,
            "max_items": max_items,
            "do_download": do_download,
            "download_langs": download_langs,
            "download_prefer": download_prefer,
            "download_fmt": download_fmt,
            "user_agent": user_agent,
            "proxy_mode": proxy_mode,
            "proxy_cool_down_sec": proxy_cool_down_sec,
            "proxy_max_fails": proxy_max_fails,
            "cookiefile": cookiefile,
            "incremental_detect": incremental_detect,
            "incremental_download": incremental_download,
            "force_refresh": force_refresh,
            "batch_size": batch_size,
            "merge_bilingual": merge_bilingual,
            "detect_mode": detect_mode,
            "adaptive_concurrency": adaptive_concurrency,
            "min_workers": min_workers,
            "max_workers_cap": max_workers_cap,
            "early_stop_on_seen": early_stop_on_seen,
            "req_rate": req_rate,
            "breaker_threshold": breaker_threshold,
            "breaker_cooldown_sec": breaker_cooldown_sec,
            "dry_run": dry_run,
            "preferred_langs": preferred_langs
        }
        save_config_snapshot(run_dir, final_config)
    except Exception:
        pass
    
    # æœ€ç»ˆç»Ÿè®¡
    if progress_callback:
        elapsed_total = time.perf_counter() - start_time
        avg_speed = len(results) / elapsed_total if elapsed_total > 0 else 0
        _send_progress_update(
            len(results), len(results),
            f"âœ“ å®Œæˆ: {downloaded} æˆåŠŸ, {skipped} è·³è¿‡, {failed} å¤±è´¥",
            {
                "downloaded": downloaded,
                "skipped": skipped,
                "failed": failed,
                "speed": avg_speed,
                "remaining": 0,
                "elapsed": elapsed_total
            }
        )
    # Webhook: run_end
    from .net import get_current_proxy_stats
    proxy_stats = get_current_proxy_stats() or {}
    proxy_blacklist_count = sum(1 for st in proxy_stats.values() if st.get("black"))
    
    # é”™è¯¯ç»Ÿè®¡
    error_breakdown = {}
    for r in results:
        st = r.get("status", "")
        if str(st).startswith("error"):
            error_breakdown[st] = error_breakdown.get(st, 0) + 1
    
    _fire_webhook("run_end", {
        "run_dir": run_dir,
        "stats": {
            "total": len(urls),
            "downloaded": downloaded,
            "errors": errs,
            "error_breakdown": error_breakdown
        },
        "proxies": {"blacklisted": proxy_blacklist_count},
        "warnings_count": len(warnings) if warnings else 0
    })
    
    # A2+B1: ä¿å­˜ç®¡çº¿å…ƒæ•°æ®ï¼ˆä¾›æŠ¥å‘Šä½¿ç”¨ï¼‰
    try:
        pipeline_meta = {
            "asr": asr_result,
            "cleanup": cleanup_result,
            "translation": translation_result,
            "bilingual": bilingual_result
        }
        pipeline_meta_file = Path(run_dir) / "pipeline_meta.json"
        with open(pipeline_meta_file, 'w', encoding='utf-8') as f:
            json.dump(pipeline_meta, f, ensure_ascii=False, indent=2)
        logging.info(f"[PIPELINE] å·²ä¿å­˜ç®¡çº¿å…ƒæ•°æ®: {pipeline_meta_file}")
    except Exception as e:
        logging.warning(f"[PIPELINE] ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    # Step 3 Day1: ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    if _HISTORY_AVAILABLE and not dry_run:
        try:
            report_path = generate_report(run_dir)
            logging.info(f"[REPORT] å·²ç”ŸæˆæŠ¥å‘Š: {report_path}")
            if progress_callback:
                progress_callback(-1, len(urls), f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        except Exception as e:
            logging.warning(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    return {
        "run_dir": run_dir,
        "total": len(urls),
        "downloaded": downloaded,
        "skipped": skipped,
        "failed": failed,
        "errors": failed,
        "last_seen": newest or last_seen,
        "failed_items": failed_items,  # æ·»åŠ å¤±è´¥é¡¹åˆ—è¡¨ï¼Œç”¨äºæ‰¹é‡é‡è¯•
        "asr_result": asr_result,
        "translation_result": translation_result,
        "cleanup_result": cleanup_result,
        "bilingual_result": bilingual_result
    }


def retry_failed_items(
    run_dir: str,
    cfg: dict | None = None,
    progress_callback: Callable[[dict], None] | None = None,
    stop_event: threading.Event | None = None,
    pause_event: threading.Event | None = None,
    user_agent: str = "",
    proxy_text: str = "",
    cookiefile: str = "",
) -> dict:
    """
    Day3: æ™ºèƒ½é‡è¯•å¤±è´¥é¡¹ï¼ˆä»…é‡è¯• retryable=True çš„é”™è¯¯ï¼‰
    
    Args:
        run_dir: åŸè¿è¡Œç›®å½•ï¼ˆè¯»å– history.jsonlï¼‰
        cfg: é‡è¯•é…ç½®ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆdict æ ¼å¼ï¼‰
        stop_event: åœæ­¢äº‹ä»¶
        pause_event: æš‚åœäº‹ä»¶
        user_agent: User-Agent
        proxy_text: ä»£ç†åˆ—è¡¨
        cookiefile: Cookie æ–‡ä»¶è·¯å¾„
    
    Returns:
        {"retried": N, "recovered": M, "run_dir": "<new_run_dir>", "new_errors": X}
    """
    import random
    from pathlib import Path
    
    # é»˜è®¤é…ç½®
    if cfg is None:
        cfg = {
            "enabled": True,
            "max_attempts": 2,
            "backoff": {"base_seconds": 5, "factor": 2.0, "jitter": True},
            "only_retryable": True,
            "filters": {
                "error_class": ["network", "rate_limit"],
                "error_code": [],
                "langs": []
            }
        }
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨
    if not cfg.get("enabled", True):
        return {"retried": 0, "recovered": 0, "run_dir": run_dir, "new_errors": 0}
    
    # è¯»å–å†å²è®°å½•
    run_dir_path = Path(run_dir)
    if not _HISTORY_AVAILABLE:
        logging.warning("history_schema ä¸å¯ç”¨ï¼Œæ— æ³•é‡è¯•")
        return {"retried": 0, "recovered": 0, "run_dir": run_dir, "new_errors": 0}
    
    history_rows = read_history(run_dir_path, limit=0)  # è¯»å–å…¨éƒ¨
    if not history_rows:
        logging.warning(f"è¿è¡Œç›®å½• {run_dir} æ— å†å²è®°å½•")
        return {"retried": 0, "recovered": 0, "run_dir": run_dir, "new_errors": 0}
    
    # ç­›é€‰å¯é‡è¯•çš„é”™è¯¯
    retry_candidates = []
    for row in history_rows:
        status = row.get("status", "")
        if status != "error":
            continue
        
        # only_retryable æ£€æŸ¥
        if cfg.get("only_retryable", True) and not row.get("retryable", False):
            continue
        
        # è¿‡æ»¤å™¨ï¼šerror_class
        error_class_filter = cfg.get("filters", {}).get("error_class", [])
        if error_class_filter:
            error_class = row.get("error_class", "")
            if error_class not in error_class_filter:
                continue
        
        # è¿‡æ»¤å™¨ï¼šerror_code
        error_code_filter = cfg.get("filters", {}).get("error_code", [])
        if error_code_filter:
            error_code = row.get("error_code", "")
            if error_code not in error_code_filter:
                continue
        
        # è¿‡æ»¤å™¨ï¼šlangs
        langs_filter = cfg.get("filters", {}).get("langs", [])
        if langs_filter:
            langs = row.get("langs", [])
            if not any(lang in langs_filter for lang in langs):
                continue
        
        retry_candidates.append(row)
    
    if not retry_candidates:
        logging.info("æ²¡æœ‰ç¬¦åˆé‡è¯•æ¡ä»¶çš„å¤±è´¥é¡¹")
        return {"retried": 0, "recovered": 0, "run_dir": run_dir, "new_errors": 0}
    
    # åˆ›å»ºæ–°çš„è¿è¡Œç›®å½•
    output_root = run_dir_path.parent
    new_run_dir = _run_dir(str(output_root))
    
    # æ„å»ºä»£ç†æ± å’Œé™æµå™¨
    pool = build_proxy_pool(proxy_text, mode="round_robin")
    limiter = RateLimiter(4.0, 8) if True else None
    breaker = CircuitBreaker(8, 120.0)
    
    # é‡è¯•å‚æ•°
    max_attempts = cfg.get("max_attempts", 2)
    backoff_config = cfg.get("backoff", {})
    base_seconds = backoff_config.get("base_seconds", 5)
    factor = backoff_config.get("factor", 2.0)
    use_jitter = backoff_config.get("jitter", True)
    
    retried_count = 0
    recovered_count = 0
    new_errors_count = 0
    
    total_items = len(retry_candidates)
    
    # å¼€å§‹é‡è¯•
    for idx, row in enumerate(retry_candidates):
        # æ£€æŸ¥åœæ­¢ä¿¡å·
        if stop_event and stop_event.is_set():
            logging.info("é‡è¯•è¢«åœæ­¢")
            break
        
        # æ£€æŸ¥æš‚åœä¿¡å·
        if pause_event:
            while pause_event.is_set():
                time.sleep(0.1)
                if stop_event and stop_event.is_set():
                    break
        
        video_id = row.get("video_id", "")
        url = row.get("url", "")
        
        if progress_callback:
            progress_callback({
                "phase": "retry",
                "current": idx + 1,
                "total": total_items,
                "message": f"é‡è¯• {idx + 1}/{total_items}: {video_id}"
            })
        
        # å°è¯•é‡æ–°æ£€æµ‹
        success = False
        last_error = None
        
        for attempt in range(1, max_attempts + 1):
            # æ£€æŸ¥åœæ­¢/æš‚åœ
            if stop_event and stop_event.is_set():
                break
            if pause_event:
                while pause_event.is_set():
                    time.sleep(0.1)
                    if stop_event and stop_event.is_set():
                        break
            
            try:
                # é‡æ–°æ£€æµ‹å­—å¹•
                from .detection import detect_links
                
                result = detect_links(
                    [url], max_workers=1, sleep_between=0.5, retry_times=1,
                    user_agent=user_agent, proxy_pool=pool, cookiefile=cookiefile,
                    stop_event=stop_event, pause_event=pause_event,
                    rate_limiter=limiter, circuit_breaker=breaker
                )
                
                if result and len(result) > 0:
                    r = result[0]
                    new_status = r.get("status", "error")
                    
                    # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
                    if new_status == "has_subs":
                        success = True
                        recovered_count += 1
                        
                        # å†™å…¥æˆåŠŸè®°å½•
                        meta = r.get("meta") or {}
                        history_row: HistoryRow = {
                            "video_id": video_id,
                            "url": url,
                            "title": meta.get("title", row.get("title", "")),
                            "channel": meta.get("channel") or meta.get("uploader") or row.get("channel", ""),
                            "status": "ok",
                            "error_code": "",
                            "error_msg": "",
                            "error_class": "",
                            "retryable": False,
                            "langs": (r.get("manual_langs") or []) + (r.get("auto_langs") or []),
                            "upload_date": meta.get("upload_date", row.get("upload_date", "")),
                            "duration": meta.get("duration", row.get("duration", 0)),
                            "view_count": meta.get("view_count", row.get("view_count", 0)),
                        }
                        write_history(new_run_dir, history_row)
                        break
                    else:
                        # ä»ç„¶å¤±è´¥
                        last_error = r.get("api_err", "unknown error")
                
            except Exception as e:
                last_error = str(e)
            
            # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œæ‰§è¡Œé€€é¿
            if not success and attempt < max_attempts:
                backoff_time = base_seconds * (factor ** (attempt - 1))
                if use_jitter:
                    backoff_time *= (1.0 + random.uniform(-0.2, 0.2))
                
                logging.info(f"é‡è¯•å¤±è´¥ï¼Œç­‰å¾… {backoff_time:.1f}s åé‡è¯• (attempt {attempt}/{max_attempts})")
                time.sleep(backoff_time)
        
        retried_count += 1
        
        # å¦‚æœæœ€ç»ˆå¤±è´¥ï¼Œå†™å…¥å¤±è´¥è®°å½•
        if not success:
            new_errors_count += 1
            
            # ä½¿ç”¨ v2 åˆ†ç±»
            status, error_code, error_msg_simplified, error_class, retryable = classify_status_v2(last_error, False)
            
            history_row: HistoryRow = {
                "video_id": video_id,
                "url": url,
                "title": row.get("title", ""),
                "channel": row.get("channel", ""),
                "status": "error",
                "error_code": error_code,
                "error_msg": error_msg_simplified,
                "error_class": error_class,
                "retryable": retryable,
                "langs": row.get("langs", []),
                "upload_date": row.get("upload_date", ""),
                "duration": row.get("duration", 0),
                "view_count": row.get("view_count", 0),
            }
            write_history(new_run_dir, history_row)
    
    # ç”ŸæˆæŠ¥å‘Š
    if _HISTORY_AVAILABLE:
        try:
            report_path = generate_report(new_run_dir)
            logging.info(f"[RETRY REPORT] å·²ç”Ÿæˆé‡è¯•æŠ¥å‘Š: {report_path}")
        except Exception as e:
            logging.warning(f"é‡è¯•æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    return {
        "retried": retried_count,
        "recovered": recovered_count,
        "run_dir": new_run_dir,
        "new_errors": new_errors_count
    }


def run_subscription_batch(
    sub_ids: list[str],
    cfg: dict,
    progress_callback: Callable[[dict], None] | None = None,
    stop_event: threading.Event | None = None,
    pause_event: threading.Event | None = None,
) -> dict:
    """
    Day4: æ‰¹é‡æ‰§è¡Œè®¢é˜…ä»»åŠ¡
    
    Args:
        sub_ids: è®¢é˜… ID åˆ—è¡¨
        cfg: å®Œæ•´é…ç½®å­—å…¸
        progress_callback: è¿›åº¦å›è°ƒï¼ˆdict æ ¼å¼ï¼‰
        stop_event: åœæ­¢äº‹ä»¶
        pause_event: æš‚åœäº‹ä»¶
    
    Returns:
        {
            "batch_dir": str,
            "runs": [{"sub_id": str, "run_dir": str, "ok": int, "error": int, "status": str}, ...]
        }
    """
    output_root = cfg.get("run", {}).get("output_root", "out")
    Path(output_root).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
    batch_ts = time.strftime("%Y%m%d_%H%M%S")
    batch_dir = Path(output_root) / f"batch_{batch_ts}"
    batch_dir.mkdir(parents=True, exist_ok=True)
    
    subscriptions = cfg.get("subscriptions", [])
    sub_dict = {sub["id"]: sub for sub in subscriptions}
    
    runs = []
    
    for idx, sub_id in enumerate(sub_ids):
        # æ£€æŸ¥åœæ­¢ä¿¡å·
        if stop_event and stop_event.is_set():
            logging.info(f"æ‰¹æ¬¡æ‰§è¡Œè¢«åœæ­¢ï¼Œå·²å®Œæˆ {idx}/{len(sub_ids)}")
            break
        
        # æ£€æŸ¥æš‚åœä¿¡å·
        if pause_event:
            while pause_event.is_set():
                time.sleep(0.1)
                if stop_event and stop_event.is_set():
                    break
        
        sub = sub_dict.get(sub_id)
        if not sub:
            logging.warning(f"è®¢é˜… {sub_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            runs.append({
                "sub_id": sub_id,
                "run_dir": "",
                "ok": 0,
                "error": 1,
                "status": "error_not_found"
            })
            continue
        
        if not sub.get("enabled", True):
            logging.info(f"è®¢é˜… {sub_id} å·²ç¦ç”¨ï¼Œè·³è¿‡")
            runs.append({
                "sub_id": sub_id,
                "run_dir": "",
                "ok": 0,
                "error": 0,
                "status": "skipped_disabled"
            })
            continue
        
        # æ„é€ è¿è¡Œå‚æ•°
        sub_type = sub.get("type", "channel")
        sub_url = sub.get("url", "")
        sub_langs = sub.get("langs", []) or cfg.get("run", {}).get("download_langs", ["zh", "en"])
        
        if progress_callback:
            progress_callback({
                "phase": "subscription",
                "current": idx + 1,
                "total": len(sub_ids),
                "message": f"æ‰§è¡Œè®¢é˜… {idx + 1}/{len(sub_ids)}: {sub.get('name', sub_id)}"
            })
        
        logging.info(f"[SUBSCRIPTION] å¼€å§‹æ‰§è¡Œ: {sub_id} ({sub.get('name', '')}) - {sub_url}")
        
        try:
            # è°ƒç”¨ run_full_process
            run_args = {
                "channel_or_playlist_url": sub_url if sub_type in ["channel", "playlist"] else "",
                "urls_override": [sub_url] if sub_type == "video" else None,
                "output_root": str(batch_dir / sub_id),
                "download_langs": sub_langs,
                "max_workers": cfg.get("run", {}).get("max_workers", 8),
                "do_download": True,
                "download_fmt": cfg.get("run", {}).get("download_fmt", "txt"),
                "progress_callback": None,  # ä¸ä¼ é€’è¿›åº¦å›è°ƒï¼Œé¿å…æ··ä¹±
                "stop_event": stop_event,
                "pause_event": pause_event,
                "user_agent": cfg.get("run", {}).get("user_agent", ""),
                "proxy_text": cfg.get("run", {}).get("proxy_text", ""),
                "cookiefile": cfg.get("run", {}).get("cookiefile", ""),
                "incremental_detect": cfg.get("run", {}).get("incremental_detect", True),
                "incremental_download": cfg.get("run", {}).get("incremental_download", True),
            }
            
            result = run_full_process(**run_args)
            
            runs.append({
                "sub_id": sub_id,
                "sub_name": sub.get("name", ""),
                "run_dir": result.get("run_dir", ""),
                "ok": result.get("total", 0) - result.get("errors", 0),
                "error": result.get("errors", 0),
                "total": result.get("total", 0),
                "status": "ok"
            })
            
            # Day4C: é‡æ–°ç”ŸæˆæŠ¥å‘Šï¼Œé™„å¸¦è®¢é˜…æ¥æºä¿¡æ¯
            run_dir = result.get("run_dir", "")
            if run_dir and _HISTORY_AVAILABLE:
                try:
                    subscription_info = {
                        "source": "subscription",
                        "sub_id": sub_id,
                        "sub_name": sub.get("name", "")
                    }
                    generate_report(run_dir, subscription_info)
                    logging.info(f"[SUBSCRIPTION] å·²ç”Ÿæˆå¸¦è®¢é˜…æ¥æºçš„æŠ¥å‘Š: {run_dir}/report.html")
                except Exception as e:
                    logging.warning(f"[SUBSCRIPTION] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            
            logging.info(f"[SUBSCRIPTION] å®Œæˆ: {sub_id} - {result.get('total', 0)} ä¸ªè§†é¢‘")
            
        except Exception as e:
            logging.error(f"[SUBSCRIPTION] å¤±è´¥: {sub_id} - {e}")
            runs.append({
                "sub_id": sub_id,
                "sub_name": sub.get("name", ""),
                "run_dir": "",
                "ok": 0,
                "error": 1,
                "total": 0,
                "status": f"error: {str(e)[:100]}"
            })
    
    # ç”Ÿæˆæ‰¹æ¬¡æ±‡æ€»æŠ¥å‘Š
    summary_path = batch_dir / "batch_summary.json"
    summary = {
        "batch_dir": str(batch_dir),
        "batch_ts": batch_ts,
        "total_subs": len(sub_ids),
        "completed_subs": len([r for r in runs if r["status"] == "ok"]),
        "failed_subs": len([r for r in runs if r["status"].startswith("error")]),
        "runs": runs
    }
    
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    logging.info(f"[BATCH] æ‰¹æ¬¡å®Œæˆ: {batch_dir}")
    
    return summary


def scheduler_tick(
    cfg: dict,
    progress_callback: Callable[[dict], None] | None = None,
    stop_event: threading.Event | None = None,
    pause_event: threading.Event | None = None,
    root: str = "."
) -> dict:
    """
    Day4: æ‰§è¡Œä¸€æ¬¡è°ƒåº¦ tick
    
    Args:
        cfg: å®Œæ•´é…ç½®å­—å…¸
        progress_callback: è¿›åº¦å›è°ƒ
        stop_event: åœæ­¢äº‹ä»¶
        pause_event: æš‚åœäº‹ä»¶
        root: æ ¹ç›®å½•
    
    Returns:
        {
            "tick_ts": str,
            "due_jobs": int,
            "executed_jobs": int,
            "results": [{"job_id": str, "status": str, "batch_dir": str, ...}, ...]
        }
    """
    import scheduler_logic
    
    if not cfg.get("scheduler", {}).get("enabled", True):
        logging.info("[SCHEDULER] è°ƒåº¦å™¨å·²ç¦ç”¨")
        return {
            "tick_ts": time.strftime("%Y-%m-%d %H:%M:%S"),
            "due_jobs": 0,
            "executed_jobs": 0,
            "results": []
        }
    
    # æ‰¾å‡ºåˆ°æœŸä»»åŠ¡
    due = scheduler_logic.due_jobs(cfg, None, root)
    
    if not due:
        logging.info("[SCHEDULER] æ— åˆ°æœŸä»»åŠ¡")
        return {
            "tick_ts": time.strftime("%Y-%m-%d %H:%M:%S"),
            "due_jobs": 0,
            "executed_jobs": 0,
            "results": []
        }
    
    logging.info(f"[SCHEDULER] æ‰¾åˆ° {len(due)} ä¸ªåˆ°æœŸä»»åŠ¡")
    
    results = []
    
    for job in due:
        job_id = job.get("id")
        sub_ids = job.get("sub_ids", [])
        
        if not sub_ids:
            logging.warning(f"[SCHEDULER] ä»»åŠ¡ {job_id} æ²¡æœ‰è®¢é˜…ï¼Œè·³è¿‡")
            continue
        
        # å°è¯•é”å®š
        if not scheduler_logic.lock_and_mark(job_id, root):
            logging.warning(f"[SCHEDULER] ä»»åŠ¡ {job_id} å·²è¢«é”å®šï¼Œè·³è¿‡")
            results.append({
                "job_id": job_id,
                "status": "skipped_locked",
                "batch_dir": ""
            })
            continue
        
        logging.info(f"[SCHEDULER] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {job_id}")
        
        if progress_callback:
            progress_callback({
                "phase": "scheduler",
                "current": len(results) + 1,
                "total": len(due),
                "message": f"æ‰§è¡Œè°ƒåº¦ä»»åŠ¡: {job_id}"
            })
        
        try:
            # æ‰§è¡Œè®¢é˜…æ‰¹æ¬¡
            batch_result = run_subscription_batch(
                sub_ids=sub_ids,
                cfg=cfg,
                progress_callback=progress_callback,
                stop_event=stop_event,
                pause_event=pause_event
            )
            
            # ä¸ºæ¯ä¸ªè®¢é˜…ç”ŸæˆæŠ¥å‘Š
            for run in batch_result.get("runs", []):
                run_dir = run.get("run_dir")
                if run_dir and Path(run_dir).exists():
                    try:
                        if _HISTORY_AVAILABLE:
                            generate_report(run_dir)
                            logging.info(f"[SCHEDULER] å·²ç”ŸæˆæŠ¥å‘Š: {run_dir}/report.html")
                    except Exception as e:
                        logging.warning(f"[SCHEDULER] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            
            # é‡Šæ”¾é”
            scheduler_logic.release(job_id, "ok", root)
            
            results.append({
                "job_id": job_id,
                "status": "ok",
                "batch_dir": batch_result.get("batch_dir", ""),
                "completed_subs": batch_result.get("completed_subs", 0),
                "failed_subs": batch_result.get("failed_subs", 0)
            })
            
            logging.info(f"[SCHEDULER] ä»»åŠ¡å®Œæˆ: {job_id}")
            
        except Exception as e:
            logging.error(f"[SCHEDULER] ä»»åŠ¡å¤±è´¥: {job_id} - {e}")
            
            # é‡Šæ”¾é”ï¼ˆæ ‡è®°ä¸ºé”™è¯¯ï¼‰
            scheduler_logic.release(job_id, "error", root)
            
            results.append({
                "job_id": job_id,
                "status": f"error: {str(e)[:100]}",
                "batch_dir": ""
            })
    
    return {
        "tick_ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        "due_jobs": len(due),
        "executed_jobs": len([r for r in results if r["status"] == "ok"]),
        "results": results
    }
