# -*- coding: utf-8 -*-
"""
core.exports â€” CSV/Excel å¯¼å‡ºï¼ˆåˆ†æä¸åˆ†äº«ï¼‰+ CDçº¿: åŒè¯­å­—å¹•åˆå¹¶
"""
from __future__ import annotations
import json, logging, csv, re
from pathlib import Path
from typing import List, Dict, Any, Tuple

def _iter_run_records(run_dir: str):
    """è¿­ä»£ run.jsonl è®°å½•"""
    path = Path(run_dir) / "run.jsonl"
    if not path.exists():
        return
    
    try:
        for line in path.read_text("utf-8", errors="ignore").splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                yield json.loads(line)
            except:
                continue
    except Exception as e:
        logging.warning(f"è¯»å– run.jsonl å¤±è´¥ï¼š{e}")

def export_run_csv(run_dir: str) -> dict:
    """
    å¯¼å‡ºå•æ¬¡è¿è¡Œçš„ CSV
    
    è¿”å›ï¼š{"videos": str, "errors": str, "ai": str}ï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
    """
    run_path = Path(run_dir)
    if not run_path.exists():
        logging.error(f"è¿è¡Œç›®å½•ä¸å­˜åœ¨ï¼š{run_dir}")
        return {"videos": "", "errors": "", "ai": ""}
    
    # è¯»å–è®°å½•
    detect_records = []
    download_records = []
    ai_records = []
    
    for rec in _iter_run_records(run_dir):
        action = rec.get("action", "")
        if action == "detect":
            detect_records.append(rec)
        elif action == "download":
            download_records.append(rec)
    
    # è¯»å– AI ç»“æœ
    ai_dir = run_path / "ai"
    if ai_dir.exists():
        for ai_file in ai_dir.glob("*.json"):
            try:
                ai_data = json.loads(ai_file.read_text("utf-8", errors="ignore"))
                ai_records.append({
                    "video_id": ai_file.stem,
                    **ai_data
                })
            except:
                continue
    
    # å¯¼å‡º videos.csv
    videos_csv = run_path / "videos.csv"
    try:
        with videos_csv.open("w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=[
                "run_dir", "video_id", "url", "title", "upload_date", "duration",
                "has_subs", "manual_langs", "auto_langs", "all_langs", "downloaded", "ai_done"
            ])
            writer.writeheader()
            
            for rec in detect_records:
                meta = rec.get("meta", {})
                video_id = rec.get("video_id", "")
                status = rec.get("status", "")
                
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                downloaded = any(d.get("video_id") == video_id for d in download_records)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ AI ç»“æœ
                ai_done = any(a.get("video_id") == video_id for a in ai_records)
                
                writer.writerow({
                    "run_dir": run_dir,
                    "video_id": video_id,
                    "url": rec.get("url", ""),
                    "title": meta.get("title", ""),
                    "upload_date": meta.get("upload_date", ""),
                    "duration": meta.get("duration", ""),
                    "has_subs": status == "has_subs",
                    "manual_langs": ",".join(rec.get("manual_langs", [])),
                    "auto_langs": ",".join(rec.get("auto_langs", [])),
                    "all_langs": ",".join(rec.get("all_langs", [])),
                    "downloaded": downloaded,
                    "ai_done": ai_done
                })
        
        logging.info(f"å·²å¯¼å‡º videos.csvï¼š{videos_csv}")
    except Exception as e:
        logging.error(f"å¯¼å‡º videos.csv å¤±è´¥ï¼š{e}")
    
    # å¯¼å‡º errors.csv
    errors_csv = run_path / "errors.csv"
    try:
        with errors_csv.open("w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=[
                "run_dir", "video_id", "url", "error_type", "message", "ts"
            ])
            writer.writeheader()
            
            for rec in detect_records:
                status = rec.get("status", "")
                if status.startswith("error_"):
                    writer.writerow({
                        "run_dir": run_dir,
                        "video_id": rec.get("video_id", ""),
                        "url": rec.get("url", ""),
                        "error_type": status,
                        "message": rec.get("api_err", ""),
                        "ts": rec.get("ts", "")
                    })
        
        logging.info(f"å·²å¯¼å‡º errors.csvï¼š{errors_csv}")
    except Exception as e:
        logging.error(f"å¯¼å‡º errors.csv å¤±è´¥ï¼š{e}")
    
    # å¯¼å‡º ai.csv
    ai_csv = run_path / "ai.csv"
    try:
        with ai_csv.open("w", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=[
                "run_dir", "video_id", "lang", "summary_len", "keywords_cnt", "chapters_cnt",
                "provider", "model", "tokens", "cost_usd", "latency_ms"
            ])
            writer.writeheader()
            
            for ai_rec in ai_records:
                meta = ai_rec.get("meta", {})
                summary = ai_rec.get("summary", "")
                keywords = ai_rec.get("keywords", [])
                chapters = ai_rec.get("chapters", [])
                
                writer.writerow({
                    "run_dir": run_dir,
                    "video_id": ai_rec.get("video_id", ""),
                    "lang": ai_rec.get("lang", ""),
                    "summary_len": len(summary) if summary else 0,
                    "keywords_cnt": len(keywords) if keywords else 0,
                    "chapters_cnt": len(chapters) if chapters else 0,
                    "provider": meta.get("provider", ""),
                    "model": meta.get("model", ""),
                    "tokens": meta.get("tokens", 0),
                    "cost_usd": meta.get("cost_usd", 0.0),
                    "latency_ms": meta.get("latency_ms", 0)
                })
        
        logging.info(f"å·²å¯¼å‡º ai.csvï¼š{ai_csv}")
    except Exception as e:
        logging.error(f"å¯¼å‡º ai.csv å¤±è´¥ï¼š{e}")
    
    return {
        "videos": str(videos_csv) if videos_csv.exists() else "",
        "errors": str(errors_csv) if errors_csv.exists() else "",
        "ai": str(ai_csv) if ai_csv.exists() else ""
    }

def export_runs_excel(out_root: str, days: int = 7) -> str:
    """
    å¯¼å‡ºè¿‘æœŸè¿è¡Œçš„ Excel æ±‡æ€»
    
    è¿”å›ï¼šExcel æ–‡ä»¶è·¯å¾„
    """
    try:
        import openpyxl
        from openpyxl import Workbook
    except ImportError:
        logging.warning("openpyxl æœªå®‰è£…ï¼Œæ— æ³•å¯¼å‡º Excelï¼Œå°è¯•é™çº§ä¸º CSV")
        return _export_runs_csv_fallback(out_root, days)
    
    from datetime import datetime, timedelta
    
    out_path = Path(out_root)
    if not out_path.exists():
        logging.error(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼š{out_root}")
        return ""
    
    # æŸ¥æ‰¾æœ€è¿‘ N å¤©çš„è¿è¡Œç›®å½•
    cutoff = datetime.now() - timedelta(days=days)
    run_dirs = []
    
    for item in out_path.iterdir():
        if item.is_dir() and item.name.startswith("2"):
            try:
                # å°è¯•è§£æç›®å½•åä¸­çš„æ—¥æœŸ
                date_str = item.name.split("_")[0]
                run_date = datetime.strptime(date_str, "%Y%m%d")
                if run_date >= cutoff:
                    run_dirs.append(item)
            except:
                continue
    
    if not run_dirs:
        logging.warning(f"æœªæ‰¾åˆ°æœ€è¿‘ {days} å¤©çš„è¿è¡Œç›®å½•")
        return ""
    
    # åˆ›å»º Excel å·¥ä½œç°¿
    wb = Workbook()
    
    # Sheet1: æ±‡æ€»ç»Ÿè®¡
    ws_summary = wb.active
    ws_summary.title = "æ±‡æ€»ç»Ÿè®¡"
    ws_summary.append(["æŒ‡æ ‡", "æ•°å€¼"])
    
    total_videos = 0
    total_has_subs = 0
    total_downloaded = 0
    total_ai_done = 0
    error_dist = {}
    
    for run_dir in run_dirs:
        for rec in _iter_run_records(str(run_dir)):
            action = rec.get("action", "")
            if action == "detect":
                total_videos += 1
                status = rec.get("status", "")
                if status == "has_subs":
                    total_has_subs += 1
                elif status.startswith("error_"):
                    error_dist[status] = error_dist.get(status, 0) + 1
            elif action == "download":
                total_downloaded += 1
        
        # ç»Ÿè®¡ AI ç»“æœ
        ai_dir = run_dir / "ai"
        if ai_dir.exists():
            total_ai_done += len(list(ai_dir.glob("*.json")))
    
    ws_summary.append(["è¿è¡Œæ€»æ•°", len(run_dirs)])
    ws_summary.append(["è§†é¢‘æ€»æ•°", total_videos])
    ws_summary.append(["æœ‰å­—å¹•", total_has_subs])
    ws_summary.append(["å·²ä¸‹è½½", total_downloaded])
    ws_summary.append(["AI å¤„ç†", total_ai_done])
    ws_summary.append([])
    ws_summary.append(["é”™è¯¯ç±»å‹", "æ•°é‡"])
    for err, count in sorted(error_dist.items(), key=lambda x: x[1], reverse=True):
        ws_summary.append([err, count])
    
    # Sheet2: æŒ‰ç›®å½•èšåˆ
    ws_by_run = wb.create_sheet("æŒ‰è¿è¡Œç›®å½•")
    ws_by_run.append(["è¿è¡Œç›®å½•", "è§†é¢‘æ•°", "æœ‰å­—å¹•", "å·²ä¸‹è½½", "AIå¤„ç†", "é”™è¯¯æ•°"])
    
    for run_dir in sorted(run_dirs, key=lambda x: x.name, reverse=True):
        run_videos = 0
        run_has_subs = 0
        run_downloaded = 0
        run_errors = 0
        
        for rec in _iter_run_records(str(run_dir)):
            action = rec.get("action", "")
            if action == "detect":
                run_videos += 1
                status = rec.get("status", "")
                if status == "has_subs":
                    run_has_subs += 1
                elif status.startswith("error_"):
                    run_errors += 1
            elif action == "download":
                run_downloaded += 1
        
        ai_dir = run_dir / "ai"
        run_ai = len(list(ai_dir.glob("*.json"))) if ai_dir.exists() else 0
        
        ws_by_run.append([
            run_dir.name,
            run_videos,
            run_has_subs,
            run_downloaded,
            run_ai,
            run_errors
        ])
    
    # Sheet3: é”™è¯¯ Top N
    ws_errors = wb.create_sheet("é”™è¯¯è¯¦æƒ…")
    ws_errors.append(["é”™è¯¯ç±»å‹", "å‡ºç°æ¬¡æ•°", "å æ¯”"])
    
    if total_videos > 0:
        for err, count in sorted(error_dist.items(), key=lambda x: x[1], reverse=True)[:20]:
            ratio = count / total_videos * 100
            ws_errors.append([err, count, f"{ratio:.1f}%"])
    
    # ä¿å­˜
    excel_path = out_path / f"summary_{days}days.xlsx"
    wb.save(excel_path)
    
    logging.info(f"å·²å¯¼å‡º Excel æ±‡æ€»ï¼š{excel_path}")
    return str(excel_path)

def _export_runs_csv_fallback(out_root: str, days: int = 7) -> str:
    """é™çº§ä¸º CSV å¯¼å‡º"""
    from datetime import datetime, timedelta
    
    out_path = Path(out_root)
    cutoff = datetime.now() - timedelta(days=days)
    run_dirs = []
    
    for item in out_path.iterdir():
        if item.is_dir() and item.name.startswith("2"):
            try:
                date_str = item.name.split("_")[0]
                run_date = datetime.strptime(date_str, "%Y%m%d")
                if run_date >= cutoff:
                    run_dirs.append(item)
            except:
                continue
    
    if not run_dirs:
        return ""
    
    csv_path = out_path / f"summary_{days}days.csv"
    
    try:
        with csv_path.open("w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["è¿è¡Œç›®å½•", "è§†é¢‘æ•°", "æœ‰å­—å¹•", "å·²ä¸‹è½½", "AIå¤„ç†", "é”™è¯¯æ•°"])
            
            for run_dir in sorted(run_dirs, key=lambda x: x.name, reverse=True):
                run_videos = 0
                run_has_subs = 0
                run_downloaded = 0
                run_errors = 0
                
                for rec in _iter_run_records(str(run_dir)):
                    action = rec.get("action", "")
                    if action == "detect":
                        run_videos += 1
                        status = rec.get("status", "")
                        if status == "has_subs":
                            run_has_subs += 1
                        elif status.startswith("error_"):
                            run_errors += 1
                    elif action == "download":
                        run_downloaded += 1
                
                ai_dir = run_dir / "ai"
                run_ai = len(list(ai_dir.glob("*.json"))) if ai_dir.exists() else 0
                
                writer.writerow([
                    run_dir.name,
                    run_videos,
                    run_has_subs,
                    run_downloaded,
                    run_ai,
                    run_errors
                ])
        
        logging.info(f"å·²å¯¼å‡º CSV æ±‡æ€»ï¼ˆé™çº§ï¼‰ï¼š{csv_path}")
        return str(csv_path)
    
    except Exception as e:
        logging.error(f"å¯¼å‡º CSV å¤±è´¥ï¼š{e}")
        return ""


# ============================================================
# CDçº¿: åŒè¯­å­—å¹•è‡ªåŠ¨åˆå¹¶
# ============================================================

def _parse_srt_file(srt_path: Path) -> List[Tuple[str, str, str]]:
    """
    è§£æ SRT æ–‡ä»¶
    
    è¿”å›: [(index, timestamp, text), ...]
    """
    try:
        content = srt_path.read_text(encoding="utf-8", errors="ignore")
        blocks = content.strip().split("\n\n")
        
        result = []
        for block in blocks:
            lines = block.strip().split("\n")
            if len(lines) >= 3:
                index = lines[0].strip()
                timestamp = lines[1].strip()
                text = "\n".join(lines[2:]).strip()
                result.append((index, timestamp, text))
        
        return result
    except Exception as e:
        logging.warning(f"è§£æ SRT æ–‡ä»¶å¤±è´¥ {srt_path.name}: {e}")
        return []


def _merge_bilingual_tsv(
    primary_file: Path,
    secondary_file: Path,
    output_file: Path
) -> bool:
    """
    åˆå¹¶åŒè¯­å­—å¹•ä¸º TSV æ ¼å¼ï¼ˆç”Ÿäº§ç¯å¢ƒé¦–é€‰ï¼‰
    
    æ ¼å¼: Index\tTimestamp\tPrimary\tSecondary
    """
    try:
        primary_blocks = _parse_srt_file(primary_file)
        secondary_blocks = _parse_srt_file(secondary_file)
        
        if not primary_blocks or not secondary_blocks:
            logging.warning(f"ç©ºå­—å¹•æ–‡ä»¶: {primary_file.name} or {secondary_file.name}")
            return False
        
        # å¯¹é½ï¼šæŒ‰ç´¢å¼•åŒ¹é…ï¼ˆç®€å•ç­–ç•¥ï¼‰
        min_len = min(len(primary_blocks), len(secondary_blocks))
        
        with output_file.open("w", encoding="utf-8", newline="") as f:
            # å†™å…¥ TSV å¤´
            f.write("Index\tTimestamp\tPrimary\tSecondary\n")
            
            for i in range(min_len):
                idx, ts, primary_text = primary_blocks[i]
                _, _, secondary_text = secondary_blocks[i]
                
                # TSV è¡Œï¼ˆæ›¿æ¢ Tab å’Œæ¢è¡Œä¸ºç©ºæ ¼ï¼‰
                primary_clean = primary_text.replace("\t", " ").replace("\n", " ")
                secondary_clean = secondary_text.replace("\t", " ").replace("\n", " ")
                
                f.write(f"{idx}\t{ts}\t{primary_clean}\t{secondary_clean}\n")
        
        logging.info(f"[BILINGUAL] TSV åˆå¹¶æˆåŠŸ: {output_file.name} ({min_len} è¡Œ)")
        return True
        
    except Exception as e:
        logging.error(f"TSV åˆå¹¶å¤±è´¥: {e}")
        return False


def _merge_bilingual_html(
    primary_file: Path,
    secondary_file: Path,
    output_file: Path,
    video_title: str = ""
) -> bool:
    """
    åˆå¹¶åŒè¯­å­—å¹•ä¸º HTML æ ¼å¼ï¼ˆå¯ç›´æ¥é˜…è¯»é¢„è§ˆï¼‰
    """
    try:
        primary_blocks = _parse_srt_file(primary_file)
        secondary_blocks = _parse_srt_file(secondary_file)
        
        if not primary_blocks or not secondary_blocks:
            return False
        
        min_len = min(len(primary_blocks), len(secondary_blocks))
        
        html_parts = [
            "<!DOCTYPE html>",
            "<html>",
            "<head>",
            '<meta charset="UTF-8">',
            f"<title>{video_title or 'Bilingual Subtitle'}</title>",
            "<style>",
            "body { font-family: Arial, sans-serif; max-width: 1000px; margin: 40px auto; padding: 20px; background: #f5f5f5; }",
            ".subtitle-pair { background: white; margin: 15px 0; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }",
            ".timestamp { color: #666; font-size: 14px; margin-bottom: 10px; }",
            ".primary { font-size: 18px; color: #1a1a1a; margin-bottom: 10px; line-height: 1.6; }",
            ".secondary { font-size: 16px; color: #0066cc; line-height: 1.5; }",
            ".index { display: inline-block; background: #007bff; color: white; padding: 2px 8px; border-radius: 3px; font-size: 12px; margin-right: 10px; }",
            "</style>",
            "</head>",
            "<body>",
            f"<h1>ğŸ“– åŒè¯­å­—å¹• - {video_title}</h1>",
            f"<p style='color: #666;'>å…± {min_len} æ®µå¯¹ç…§</p>",
        ]
        
        for i in range(min_len):
            idx, ts, primary_text = primary_blocks[i]
            _, _, secondary_text = secondary_blocks[i]
            
            html_parts.extend([
                '<div class="subtitle-pair">',
                f'<div class="timestamp"><span class="index">{idx}</span>{ts}</div>',
                f'<div class="primary">{primary_text}</div>',
                f'<div class="secondary">{secondary_text}</div>',
                '</div>',
            ])
        
        html_parts.extend([
            "</body>",
            "</html>"
        ])
        
        output_file.write_text("\n".join(html_parts), encoding="utf-8")
        logging.info(f"[BILINGUAL] HTML åˆå¹¶æˆåŠŸ: {output_file.name}")
        return True
        
    except Exception as e:
        logging.error(f"HTML åˆå¹¶å¤±è´¥: {e}")
        return False


def export_bilingual_subtitles(
    run_dir: str,
    primary_lang: str = "auto",
    secondary_lang: str = "en",
    output_format: str = "tsv",
    output_subdir: str = "bilingual"
) -> dict:
    """
    CDçº¿: å¯¼å‡ºåŒè¯­å¯¹ç…§å­—å¹•
    
    Args:
        run_dir: è¿è¡Œç›®å½•
        primary_lang: ä¸»è¯­è¨€ ("auto" = è‡ªåŠ¨æ£€æµ‹é¦–è¯­è¨€)
        secondary_lang: æ¬¡è¯­è¨€ï¼ˆå¯¹ç…§ï¼‰
        output_format: "tsv" | "html" | "txt"
        output_subdir: è¾“å‡ºå­ç›®å½•å
    
    Returns:
        {
            "total": int,           # å¤„ç†è§†é¢‘æ€»æ•°
            "success": int,         # æˆåŠŸåˆå¹¶æ•°
            "files": list[str],     # ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
            "format": str           # è¾“å‡ºæ ¼å¼
        }
    """
    result = {
        "total": 0,
        "success": 0,
        "files": [],
        "format": output_format
    }
    
    try:
        run_path = Path(run_dir)
        subs_dir = run_path / "subs"
        
        if not subs_dir.exists():
            logging.warning(f"[BILINGUAL] å­—å¹•ç›®å½•ä¸å­˜åœ¨: {subs_dir}")
            return result
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        bilingual_dir = run_path / output_subdir
        bilingual_dir.mkdir(parents=True, exist_ok=True)
        
        # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘ ID
        video_ids = set()
        for srt_file in subs_dir.glob("*.srt"):
            # æå–è§†é¢‘ ID (æ ¼å¼: <video_id>.<lang>.srt)
            match = re.search(r"([A-Za-z0-9_-]{11})\.", srt_file.name)
            if match:
                video_ids.add(match.group(1))
        
        result["total"] = len(video_ids)
        
        if not video_ids:
            logging.warning(f"[BILINGUAL] æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
            return result
        
        # é€ä¸ªè§†é¢‘å¤„ç†
        for vid in sorted(video_ids):
            # æŸ¥æ‰¾ä¸»è¯­è¨€å­—å¹•ï¼ˆæ”¯æŒè¯­è¨€å˜ä½“ï¼‰
            if primary_lang == "auto":
                # è‡ªåŠ¨æ£€æµ‹ï¼šä¼˜å…ˆçº§ zh > ja > ko > å…¶ä»–
                # æ”¯æŒæŸ¥æ‰¾ zh çš„å˜ä½“ï¼ˆå¦‚ zh-TW, zh-Hans, zh-CNï¼‰
                primary_file = None
                for lang in ["zh", "zh-TW", "zh-Hans", "zh-Hant", "zh-CN", "ja", "ko"]:
                    p = subs_dir / f"{vid}.{lang}.srt"
                    if p.exists():
                        primary_file = p
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡ glob æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å˜ä½“
                if not primary_file:
                    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ zh å˜ä½“
                    zh_variants = list(subs_dir.glob(f"{vid}.zh*.srt"))
                    if zh_variants:
                        primary_file = zh_variants[0]
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå–ç¬¬ä¸€ä¸ª
                        candidates = list(subs_dir.glob(f"{vid}.*.srt"))
                        if candidates:
                            primary_file = candidates[0]
            else:
                # æ”¯æŒæŸ¥æ‰¾è¯­è¨€å˜ä½“
                primary_file = subs_dir / f"{vid}.{primary_lang}.srt"
                if not primary_file.exists() and primary_lang.lower() == "zh":
                    # å°è¯•æŸ¥æ‰¾ zh çš„å˜ä½“
                    zh_variants = list(subs_dir.glob(f"{vid}.zh*.srt"))
                    if zh_variants:
                        primary_file = zh_variants[0]
            
            # æŸ¥æ‰¾æ¬¡è¯­è¨€å­—å¹•ï¼ˆæ”¯æŒè¯­è¨€å˜ä½“ï¼‰
            secondary_file = subs_dir / f"{vid}.{secondary_lang}.srt"
            if not secondary_file.exists() and secondary_lang.lower() == "zh":
                # å°è¯•æŸ¥æ‰¾ zh çš„å˜ä½“
                zh_variants = list(subs_dir.glob(f"{vid}.zh*.srt"))
                if zh_variants:
                    secondary_file = zh_variants[0]
            
            if not primary_file or not primary_file.exists():
                logging.debug(f"[BILINGUAL] è·³è¿‡ {vid}: æœªæ‰¾åˆ°ä¸»è¯­è¨€å­—å¹•")
                continue
            
            if not secondary_file.exists():
                logging.debug(f"[BILINGUAL] è·³è¿‡ {vid}: æœªæ‰¾åˆ°æ¬¡è¯­è¨€å­—å¹• ({secondary_lang})")
                continue
            
            # æ£€æŸ¥ä¸»è¯­è¨€å’Œæ¬¡è¯­è¨€æ˜¯å¦ç›¸åŒï¼ˆé¿å…åŒè¯­è¨€åˆå¹¶ï¼‰
            def extract_lang_from_filename(file_path: Path, video_id: str):
                """ä»æ–‡ä»¶åæå–è¯­è¨€ä»£ç """
                filename = file_path.stem  # ä¸å«æ‰©å±•å
                # ç§»é™¤è§†é¢‘IDå‰ç¼€
                if filename.startswith(video_id):
                    suffix = filename[len(video_id):].lstrip('.')
                    parts = suffix.split('.')
                    if parts:
                        # å¤„ç†é‡å¤è¯­è¨€ä»£ç ï¼ˆå¦‚ en.en -> enï¼‰
                        lang_code = parts[-1]
                        # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è¯­è¨€ä»£ç ï¼ˆæ”¯æŒå¸¦è¿å­—ç¬¦çš„ä»£ç ï¼Œå¦‚ zh-TWï¼‰
                        if len(lang_code) <= 8 and (lang_code.replace('-', '').isalpha() or lang_code.replace('_', '').isalpha()):
                            return lang_code
                return None
            
            def normalize_lang_code(lang_code: str) -> str:
                """è§„èŒƒåŒ–è¯­è¨€ä»£ç ï¼Œå°†å˜ä½“ï¼ˆå¦‚ zh-TWï¼‰æ˜ å°„åˆ°ä¸»è¯­è¨€ï¼ˆå¦‚ zhï¼‰"""
                if not lang_code:
                    return ""
                lang_lower = lang_code.lower()
                # ä¸­æ–‡å˜ä½“æ˜ å°„åˆ° zh
                if lang_lower.startswith('zh-') or lang_lower.startswith('zh_'):
                    return 'zh'
                # è‹±æ–‡å˜ä½“æ˜ å°„åˆ° en
                if lang_lower.startswith('en-') or lang_lower.startswith('en_'):
                    return 'en'
                # å…¶ä»–ï¼šå–ä¸»ç ï¼ˆ- æˆ– _ ä¹‹å‰ï¼‰
                return lang_lower.split('-')[0].split('_')[0]
            
            primary_lang_detected = None
            if primary_lang == "auto":
                # ä»æ–‡ä»¶åæå–è¯­è¨€ä»£ç ï¼ˆæ”¯æŒå˜ä½“ï¼‰
                for lang in ["zh", "zh-TW", "zh-Hans", "zh-Hant", "zh-CN", "ja", "ko"]:
                    p = subs_dir / f"{vid}.{lang}.srt"
                    if p.exists() and p == primary_file:
                        primary_lang_detected = lang
                        logging.info(f"[BILINGUAL] ä¸»è¯­è¨€æ–‡ä»¶åŒ¹é…: {lang} -> {p.name}")
                        break
                
                # å¦‚æœæ²¡åŒ¹é…åˆ°ï¼Œä»æ–‡ä»¶åæå–
                if not primary_lang_detected:
                    primary_lang_detected = extract_lang_from_filename(primary_file, vid)
                    if primary_lang_detected:
                        logging.info(f"[BILINGUAL] ä»æ–‡ä»¶åæå–ä¸»è¯­è¨€: {primary_lang_detected} (æ–‡ä»¶: {primary_file.name})")
            else:
                primary_lang_detected = primary_lang
            
            # æå–æ¬¡è¯­è¨€ä»£ç 
            secondary_lang_detected = secondary_lang
            if secondary_file.exists():
                detected = extract_lang_from_filename(secondary_file, vid)
                if detected:
                    secondary_lang_detected = detected
                    logging.info(f"[BILINGUAL] ä»æ–‡ä»¶åæå–æ¬¡è¯­è¨€: {detected} (æ–‡ä»¶: {secondary_file.name})")
            
            logging.info(f"[BILINGUAL] æ£€æµ‹åˆ°çš„è¯­è¨€: primary={primary_lang_detected}, secondary={secondary_lang_detected}")
            
            # å¦‚æœä¸»è¯­è¨€å’Œæ¬¡è¯­è¨€ç›¸åŒï¼ˆè§„èŒƒåŒ–åï¼‰ï¼Œè·³è¿‡åˆå¹¶
            primary_lang_normalized = normalize_lang_code(primary_lang_detected) if primary_lang_detected else None
            secondary_lang_normalized = normalize_lang_code(secondary_lang_detected) if secondary_lang_detected else None
            
            logging.info(f"[BILINGUAL] è§„èŒƒåŒ–åçš„è¯­è¨€: primary={primary_lang_normalized}, secondary={secondary_lang_normalized}")
            
            if primary_lang_normalized and secondary_lang_normalized and primary_lang_normalized == secondary_lang_normalized:
                logging.warning(f"[BILINGUAL] è·³è¿‡ {vid}: ä¸»è¯­è¨€å’Œæ¬¡è¯­è¨€ç›¸åŒ ({primary_lang_detected} -> {primary_lang_normalized}, {secondary_lang_detected} -> {secondary_lang_normalized})ï¼Œæ— æ³•ç”ŸæˆåŒè¯­å­—å¹•")
                continue
            
            # å¦‚æœä¸»è¯­è¨€æ–‡ä»¶å’Œæ¬¡è¯­è¨€æ–‡ä»¶æ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶
            if primary_file.resolve() == secondary_file.resolve():
                logging.warning(f"[BILINGUAL] è·³è¿‡ {vid}: ä¸»è¯­è¨€æ–‡ä»¶å’Œæ¬¡è¯­è¨€æ–‡ä»¶æ˜¯åŒä¸€ä¸ªæ–‡ä»¶")
                continue
            
            # åˆå¹¶å­—å¹•
            if output_format == "tsv":
                output_file = bilingual_dir / f"{vid}.bilingual.tsv"
                success = _merge_bilingual_tsv(primary_file, secondary_file, output_file)
            
            elif output_format == "html":
                output_file = bilingual_dir / f"{vid}.bilingual.html"
                # å°è¯•ä» run.jsonl è·å–è§†é¢‘æ ‡é¢˜
                video_title = vid  # é»˜è®¤ä½¿ç”¨ ID
                success = _merge_bilingual_html(primary_file, secondary_file, output_file, video_title)
            
            else:
                logging.warning(f"[BILINGUAL] ä¸æ”¯æŒçš„æ ¼å¼: {output_format}")
                continue
            
            if success:
                result["success"] += 1
                result["files"].append(str(output_file.relative_to(run_path)))
                logging.info(f"[BILINGUAL] âœ“ {vid}: æˆåŠŸç”ŸæˆåŒè¯­å­—å¹• ({output_file.name})")
            else:
                logging.warning(f"[BILINGUAL] âœ— {vid}: åŒè¯­å­—å¹•ç”Ÿæˆå¤±è´¥")
        
        logging.info(f"[BILINGUAL] åŒè¯­å­—å¹•å¯¼å‡ºå®Œæˆ: {result['success']}/{result['total']} (æ ¼å¼={output_format})")
        
    except Exception as e:
        logging.error(f"[BILINGUAL] åŒè¯­å­—å¹•å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)
    
    return result

